{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Lambda, Embedding\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from corpus_WiNER.corpus_utils import *\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP \n",
    "\n",
    "Utilizando la estrategia de decaimiento exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_train_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_train_exp_decay_W_5.npz')\n",
    "X_train = word_vectors.items()[0][1][:100000]\n",
    "y_train = entity_vector.items()[0][1][:100000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_dev_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_dev_exp_decay_W_5.npz')\n",
    "X_dev = word_vectors.items()[0][1][:20000]\n",
    "y_dev = entity_vector.items()[0][1][:20000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_test_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_test_exp_decay_W_5.npz')\n",
    "X_test = word_vectors.items()[0][1][:20000]\n",
    "y_test = entity_vector.items()[0][1][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word vectors: 100000\n",
      "# non entities 55825\n",
      "# word vectors: 20000\n",
      "# non entities 9634\n",
      "# word vectors: 20000\n",
      "# non entities 10410\n"
     ]
    }
   ],
   "source": [
    "print('# word vectors:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# word vectors:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# word vectors:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 2, 2, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes1 = 300\n",
    "nodes2 = 512\n",
    "lr = 0.001\n",
    "l2 = 0.01\n",
    "drop = 0.1\n",
    "\n",
    "model = Sequential()    \n",
    "model.add(Dense(nodes1,\n",
    "                input_shape=(300,),\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)\n",
    "         )\n",
    ")      \n",
    "model.add(Dropout(drop))   \n",
    "\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "\n",
    "model.add(Dropout(drop))\n",
    "model.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(5, activation = 'softmax')) # PER - LOC - ORG - MISC - O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 737,729\n",
      "Trainable params: 737,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adadelta(lr = lr),\n",
    "              loss = categorical_crossentropy,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 43us/step - loss: 21.1002 - acc: 0.1901 - val_loss: 21.0348 - val_acc: 0.3146\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.9520 - acc: 0.4330 - val_loss: 20.8860 - val_acc: 0.4757\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 20.7924 - acc: 0.5508 - val_loss: 20.7286 - val_acc: 0.4817\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.6237 - acc: 0.5582 - val_loss: 20.5644 - val_acc: 0.4817\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 20.4466 - acc: 0.5582 - val_loss: 20.3952 - val_acc: 0.4817\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.2633 - acc: 0.5582 - val_loss: 20.2245 - val_acc: 0.4817\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.0786 - acc: 0.5583 - val_loss: 20.0587 - val_acc: 0.4817\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.9006 - acc: 0.5582 - val_loss: 19.9039 - val_acc: 0.4817\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.7362 - acc: 0.5583 - val_loss: 19.7597 - val_acc: 0.4817\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.5839 - acc: 0.5583 - val_loss: 19.6206 - val_acc: 0.4817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN \n",
    "\n",
    "Utilizando ventana sim√©trica de palabras que rodea a la objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['', '', 'Watching', 'Ellie', 'is']</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['', 'Watching', 'Ellie', 'is', 'an']</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Watching', 'Ellie', 'is', 'an', 'American']</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['is', 'an', 'American', 'sitcom', 'that']</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['that', 'starred', 'Julia', 'Louis-Dreyfus', ...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words entityType\n",
       "0                ['', '', 'Watching', 'Ellie', 'is']       MISC\n",
       "1              ['', 'Watching', 'Ellie', 'is', 'an']       MISC\n",
       "2      ['Watching', 'Ellie', 'is', 'an', 'American']          O\n",
       "3         ['is', 'an', 'American', 'sitcom', 'that']        LOC\n",
       "4  ['that', 'starred', 'Julia', 'Louis-Dreyfus', ...        PER"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_train.csv')\n",
    "data_dev = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_dev.csv')\n",
    "data_test = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_test.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['words'].values[:100000]\n",
    "y_train = data_train['entityType'].values[:100000]\n",
    "X_dev = data_dev['words'].values[:20000]\n",
    "y_dev = data_dev['entityType'].values[:20000]\n",
    "X_test = data_test['words'].values[:20000]\n",
    "y_test = data_test['entityType'].values[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances: 100000\n",
      "# non entities 54508\n",
      "# instances: 20000\n",
      "# non entities 9590\n",
      "# instances: 20000\n",
      "# non entities 10856\n"
     ]
    }
   ],
   "source": [
    "print('# instances:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# instances:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# instances:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 1, 0, 0, 0, 0, 4, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in ast.literal_eval(instance):\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('./models/google/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)\n",
    "X_dev = transform_input(X_dev, w2v_model.vocab)\n",
    "X_test = transform_input(X_test, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # For mini-batch gradient descent\n",
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "epochs = 10\n",
    "len_words = 5\n",
    "input_size = len_words # amount of words by row\n",
    "train_examples = len(X_train)\n",
    "test_examples = len(X_test)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d\n",
    "steps = 5 # number of words in the sentence\n",
    "channels = 1\n",
    "input_shape = (steps, channels) #3D tensor with shape: `(batch, steps, channels)`\n",
    "# # Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "X_train = np.asarray(X_train)\n",
    "X_dev = np.asarray(X_dev)\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0, 14985, 28491,     4],\n",
       "       [    0, 14985, 28491,     4,    27],\n",
       "       [14985, 28491,     4,    27,   259],\n",
       "       [    4,    27,   259, 17470,     3],\n",
       "       [    3,  9483, 10751,     0,     0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5)\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 10\n",
    "pool_size = 2\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 300)\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)\n",
    "print(emb.shape)\n",
    "# The output here has shape (batch_size (?), words_in_reviews (?), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "#     3D tensor with shape: `(batch, steps, channels)`\n",
    "\n",
    "# Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "#     `steps` value might have changed due to padding or strides.\n",
    "\n",
    "# Specify each convolution layer and their kernel size i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=2, activation='relu')(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "maxp1_1 = MaxPooling1D(pool_size=pool_size)(btch1_1)\n",
    "flat1_1 = Flatten()(maxp1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "maxp1_2 = MaxPooling1D(pool_size=pool_size)(btch1_2)\n",
    "flat1_2 = Flatten()(maxp1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "maxp1_3 = MaxPooling1D(pool_size=pool_size)(btch1_3)\n",
    "flat1_3 = Flatten()(maxp1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([flat1_1, flat1_2, flat1_3], axis=1)\n",
    "drp1 = Dropout(0)(cnct)\n",
    "\n",
    "dns1  = Dense(128, activation='relu')(drp1)\n",
    "out = Dense(num_classes, activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 5, 300)       900000000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4, 10)        6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3, 10)        9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2, 10)        12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 10)        40          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3, 10)        40          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2, 10)        40          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 10)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 10)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 10)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          5248        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            645         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 900,033,043\n",
      "Trainable params: 32,983\n",
      "Non-trainable params: 900,000,060\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 3s 29us/step - loss: 0.6258 - acc: 0.7848 - val_loss: 0.6379 - val_acc: 0.7860\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4594 - acc: 0.8433 - val_loss: 0.6186 - val_acc: 0.7912\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4102 - acc: 0.8590 - val_loss: 0.6101 - val_acc: 0.7937\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.3754 - acc: 0.8712 - val_loss: 0.6291 - val_acc: 0.7933\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.3487 - acc: 0.8800 - val_loss: 0.6388 - val_acc: 0.7926\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.3259 - acc: 0.8869 - val_loss: 0.6700 - val_acc: 0.7878\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.3067 - acc: 0.8939 - val_loss: 0.6791 - val_acc: 0.7906\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.2901 - acc: 0.9001 - val_loss: 0.7050 - val_acc: 0.7834\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.2759 - acc: 0.9044 - val_loss: 0.7343 - val_acc: 0.7835\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.2619 - acc: 0.9098 - val_loss: 0.7498 - val_acc: 0.7850\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('./models/cnn_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.5_l2_1....</td>\n",
       "      <td>0.173732</td>\n",
       "      <td>0.973376</td>\n",
       "      <td>0.96393</td>\n",
       "      <td>0.80485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cnn_num_filters_10_pool_size_1_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.418569</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>0.86466</td>\n",
       "      <td>0.80480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.5_l2_0....</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>0.993586</td>\n",
       "      <td>0.96606</td>\n",
       "      <td>0.80470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>0.876053</td>\n",
       "      <td>0.94491</td>\n",
       "      <td>0.80460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cnn_num_filters_10_pool_size_1_drop_0.5_l2_1.0...</td>\n",
       "      <td>0.436353</td>\n",
       "      <td>0.634725</td>\n",
       "      <td>0.86098</td>\n",
       "      <td>0.80265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.5_l2_1.0...</td>\n",
       "      <td>0.227528</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.94090</td>\n",
       "      <td>0.80250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.3_l2_0....</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>1.127969</td>\n",
       "      <td>0.98377</td>\n",
       "      <td>0.80100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.3_l2_0.3...</td>\n",
       "      <td>0.122047</td>\n",
       "      <td>1.092002</td>\n",
       "      <td>0.97107</td>\n",
       "      <td>0.80080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cnn_num_filters_100_pool_size_2_drop_0.5_l2_0....</td>\n",
       "      <td>0.141580</td>\n",
       "      <td>1.062206</td>\n",
       "      <td>0.96539</td>\n",
       "      <td>0.79795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn_num_filters_20_pool_size_2_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.328026</td>\n",
       "      <td>0.738538</td>\n",
       "      <td>0.89535</td>\n",
       "      <td>0.79760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           model_name  train_loss  dev_loss  \\\n",
       "32  cnn_num_filters_100_pool_size_1_drop_0.5_l2_1....    0.173732  0.973376   \n",
       "23  cnn_num_filters_10_pool_size_1_drop_0.5_l2_0.5...    0.418569  0.637613   \n",
       "31  cnn_num_filters_100_pool_size_1_drop_0.5_l2_0....    0.150977  0.993586   \n",
       "26  cnn_num_filters_50_pool_size_1_drop_0.5_l2_0.5...    0.201993  0.876053   \n",
       "24  cnn_num_filters_10_pool_size_1_drop_0.5_l2_1.0...    0.436353  0.634725   \n",
       "27  cnn_num_filters_50_pool_size_1_drop_0.5_l2_1.0...    0.227528  0.851900   \n",
       "30  cnn_num_filters_100_pool_size_1_drop_0.3_l2_0....    0.084707  1.127969   \n",
       "25  cnn_num_filters_50_pool_size_1_drop_0.3_l2_0.3...    0.122047  1.092002   \n",
       "18  cnn_num_filters_100_pool_size_2_drop_0.5_l2_0....    0.141580  1.062206   \n",
       "8   cnn_num_filters_20_pool_size_2_drop_0.5_l2_0.5...    0.328026  0.738538   \n",
       "\n",
       "    train_acc  dev_acc  \n",
       "32    0.96393  0.80485  \n",
       "23    0.86466  0.80480  \n",
       "31    0.96606  0.80470  \n",
       "26    0.94491  0.80460  \n",
       "24    0.86098  0.80265  \n",
       "27    0.94090  0.80250  \n",
       "30    0.98377  0.80100  \n",
       "25    0.97107  0.80080  \n",
       "18    0.96539  0.79795  \n",
       "8     0.89535  0.79760  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_metrics = metrics.sort_values(by=['dev_acc'], ascending=False)\n",
    "sorted_metrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_num_filters_100_pool_size_1_drop_0.5_l2_1.0_batch_size_512_epochs_100\n"
     ]
    }
   ],
   "source": [
    "model_name = list(sorted_metrics['model_name'])[0]\n",
    "print(model_name)\n",
    "best_model = load_model('./models/saved/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_test.csv')\n",
    "X_test = test_data['words'].values[:20000]\n",
    "y_test = test_data['entityType'].values[:20000]\n",
    "w2v_model = KeyedVectors.load('./models/google/word2vecGoogle.model')\n",
    "X_test = np.asarray(transform_input(X_test, w2v_model.vocab))\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_test = to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1s 65us/step\n"
     ]
    }
   ],
   "source": [
    "performance = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8748557825446129\n",
      "Test accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {}'.format(performance[0]))\n",
    "print('Test accuracy: {}'.format(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
