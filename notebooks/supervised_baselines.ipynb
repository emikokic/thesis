{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from corpus_WiNER.corpus_utils import *\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Lambda, Embedding, TimeDistributed\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP \n",
    "\n",
    "Utilizando la estrategia de decaimiento exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_train_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_train_exp_decay_W_5.npz')\n",
    "X_train = word_vectors.items()[0][1][:100000]\n",
    "y_train = entity_vector.items()[0][1][:100000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_dev_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_dev_exp_decay_W_5.npz')\n",
    "X_dev = word_vectors.items()[0][1][:20000]\n",
    "y_dev = entity_vector.items()[0][1][:20000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_test_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_test_exp_decay_W_5.npz')\n",
    "X_test = word_vectors.items()[0][1][:20000]\n",
    "y_test = entity_vector.items()[0][1][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word vectors: 100000\n",
      "# non entities 55825\n",
      "# word vectors: 20000\n",
      "# non entities 9634\n",
      "# word vectors: 20000\n",
      "# non entities 10410\n"
     ]
    }
   ],
   "source": [
    "print('# word vectors:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# word vectors:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# word vectors:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 2, 2, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes1 = 300\n",
    "nodes2 = 512\n",
    "lr = 0.001\n",
    "l2 = 0.01\n",
    "drop = 0.1\n",
    "\n",
    "model = Sequential()    \n",
    "model.add(Dense(nodes1,\n",
    "                input_shape=(300,),\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)\n",
    "         )\n",
    ")      \n",
    "model.add(Dropout(drop))   \n",
    "\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "\n",
    "model.add(Dropout(drop))\n",
    "model.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(5, activation = 'softmax')) # PER - LOC - ORG - MISC - O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 737,729\n",
      "Trainable params: 737,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adadelta(lr = lr),\n",
    "              loss = categorical_crossentropy,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 43us/step - loss: 21.1002 - acc: 0.1901 - val_loss: 21.0348 - val_acc: 0.3146\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.9520 - acc: 0.4330 - val_loss: 20.8860 - val_acc: 0.4757\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 20.7924 - acc: 0.5508 - val_loss: 20.7286 - val_acc: 0.4817\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.6237 - acc: 0.5582 - val_loss: 20.5644 - val_acc: 0.4817\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 20.4466 - acc: 0.5582 - val_loss: 20.3952 - val_acc: 0.4817\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.2633 - acc: 0.5582 - val_loss: 20.2245 - val_acc: 0.4817\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.0786 - acc: 0.5583 - val_loss: 20.0587 - val_acc: 0.4817\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.9006 - acc: 0.5582 - val_loss: 19.9039 - val_acc: 0.4817\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.7362 - acc: 0.5583 - val_loss: 19.7597 - val_acc: 0.4817\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.5839 - acc: 0.5583 - val_loss: 19.6206 - val_acc: 0.4817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Wide Model\n",
    "\n",
    "Utilizando ventana simétrica de palabras que rodea a la objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['', '', '', '', '', 'Watching', 'Ellie', 'is'...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['', '', '', '', 'Watching', 'Ellie', 'is', 'a...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['', '', '', 'Watching', 'Ellie', 'is', 'an', ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['', 'Watching', 'Ellie', 'is', 'an', 'America...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['an', 'American', 'sitcom', 'that', 'starred'...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words entityType\n",
       "0  ['', '', '', '', '', 'Watching', 'Ellie', 'is'...       MISC\n",
       "1  ['', '', '', '', 'Watching', 'Ellie', 'is', 'a...       MISC\n",
       "2  ['', '', '', 'Watching', 'Ellie', 'is', 'an', ...          O\n",
       "3  ['', 'Watching', 'Ellie', 'is', 'an', 'America...        LOC\n",
       "4  ['an', 'American', 'sitcom', 'that', 'starred'...        PER"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('../corpus_WiNER/cnn_instances/words_entity_W_5_cnn_train.csv')\n",
    "data_dev = pd.read_csv('../corpus_WiNER/cnn_instances/words_entity_W_5_cnn_dev.csv')\n",
    "data_test = pd.read_csv('../corpus_WiNER/cnn_instances/words_entity_W_5_cnn_test.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['words'].values[:100000]\n",
    "y_train = data_train['entityType'].values[:100000]\n",
    "X_dev = data_dev['words'].values[:20000]\n",
    "y_dev = data_dev['entityType'].values[:20000]\n",
    "X_test = data_test['words'].values[:20000]\n",
    "y_test = data_test['entityType'].values[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances: 100000\n",
      "# non entities 54508\n",
      "# instances: 20000\n",
      "# non entities 9590\n",
      "# instances: 20000\n",
      "# non entities 10856\n"
     ]
    }
   ],
   "source": [
    "print('# instances:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# instances:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# instances:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 1, 0, 0, 0, 0, 4, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in ast.literal_eval(instance):\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('../models/google/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)\n",
    "X_dev = transform_input(X_dev, w2v_model.vocab)\n",
    "X_test = transform_input(X_test, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # For mini-batch gradient descent\n",
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "epochs = 10\n",
    "len_words = 11\n",
    "input_size = len_words # amount of words by row\n",
    "train_examples = len(X_train)\n",
    "test_examples = len(X_test)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d\n",
    "steps = 5 # number of words in the sentence\n",
    "channels = 1\n",
    "input_shape = (steps, channels) #3D tensor with shape: `(batch, steps, channels)`\n",
    "# # Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "X_train = np.asarray(X_train)\n",
    "X_dev = np.asarray(X_dev)\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0, 14985, 28491,     4,    27,\n",
       "          259, 17470],\n",
       "       [    0,     0,     0,     0, 14985, 28491,     4,    27,   259,\n",
       "        17470,     3],\n",
       "       [    0,     0,     0, 14985, 28491,     4,    27,   259, 17470,\n",
       "            3,  9483],\n",
       "       [    0, 14985, 28491,     4,    27,   259, 17470,     3,  9483,\n",
       "        10751,     0],\n",
       "       [   27,   259, 17470,     3,  9483, 10751,     0,     0,    10,\n",
       "         1088,    18]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Wide Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 11)\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 10\n",
    "pool_size = 2\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 11, 300)\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)\n",
    "print(emb.shape)\n",
    "# The output here has shape (batch_size (?), words_in_reviews (?), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "#     3D tensor with shape: `(batch, steps, channels)`\n",
    "\n",
    "# Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "#     `steps` value might have changed due to padding or strides.\n",
    "\n",
    "# Specify each convolution layer and their kernel size i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=2, activation='relu')(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "maxp1_1 = MaxPooling1D(pool_size=pool_size)(btch1_1)\n",
    "flat1_1 = Flatten()(maxp1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "maxp1_2 = MaxPooling1D(pool_size=pool_size)(btch1_2)\n",
    "flat1_2 = Flatten()(maxp1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "maxp1_3 = MaxPooling1D(pool_size=pool_size)(btch1_3)\n",
    "flat1_3 = Flatten()(maxp1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([flat1_1, flat1_2, flat1_3], axis=1)\n",
    "drp1 = Dropout(0)(cnct)\n",
    "\n",
    "dns1  = Dense(128, activation='relu')(drp1)\n",
    "out = Dense(num_classes, activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 300)      900000000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 10)       6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 9, 10)        9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 8, 10)        12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 10)       40          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 9, 10)        40          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 10)        40          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 5, 10)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 4, 10)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 4, 10)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 40)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 40)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 130)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 130)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16768       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            645         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 900,044,563\n",
      "Trainable params: 44,503\n",
      "Non-trainable params: 900,000,060\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 45us/step - loss: 0.6723 - acc: 0.7624 - val_loss: 0.6417 - val_acc: 0.7818\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.4624 - acc: 0.8403 - val_loss: 0.6147 - val_acc: 0.7924\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.4029 - acc: 0.8614 - val_loss: 0.6146 - val_acc: 0.7967\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 0.3598 - acc: 0.8762 - val_loss: 0.6494 - val_acc: 0.7862\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 0.3248 - acc: 0.8891 - val_loss: 0.6492 - val_acc: 0.7893\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.2949 - acc: 0.8977 - val_loss: 0.6966 - val_acc: 0.7839\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.2689 - acc: 0.9073 - val_loss: 0.7326 - val_acc: 0.7802\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.2478 - acc: 0.9137 - val_loss: 0.7529 - val_acc: 0.7815\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.2280 - acc: 0.9214 - val_loss: 0.8062 - val_acc: 0.7724\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 0.2098 - acc: 0.9282 - val_loss: 0.8296 - val_acc: 0.7759\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('./models/cnn_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.5_l2_1....</td>\n",
       "      <td>0.173732</td>\n",
       "      <td>0.973376</td>\n",
       "      <td>0.96393</td>\n",
       "      <td>0.80485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cnn_num_filters_10_pool_size_1_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.418569</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>0.86466</td>\n",
       "      <td>0.80480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.5_l2_0....</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>0.993586</td>\n",
       "      <td>0.96606</td>\n",
       "      <td>0.80470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>0.876053</td>\n",
       "      <td>0.94491</td>\n",
       "      <td>0.80460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cnn_num_filters_10_pool_size_1_drop_0.5_l2_1.0...</td>\n",
       "      <td>0.436353</td>\n",
       "      <td>0.634725</td>\n",
       "      <td>0.86098</td>\n",
       "      <td>0.80265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.5_l2_1.0...</td>\n",
       "      <td>0.227528</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.94090</td>\n",
       "      <td>0.80250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cnn_num_filters_100_pool_size_1_drop_0.3_l2_0....</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>1.127969</td>\n",
       "      <td>0.98377</td>\n",
       "      <td>0.80100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn_num_filters_50_pool_size_1_drop_0.3_l2_0.3...</td>\n",
       "      <td>0.122047</td>\n",
       "      <td>1.092002</td>\n",
       "      <td>0.97107</td>\n",
       "      <td>0.80080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cnn_num_filters_100_pool_size_2_drop_0.5_l2_0....</td>\n",
       "      <td>0.141580</td>\n",
       "      <td>1.062206</td>\n",
       "      <td>0.96539</td>\n",
       "      <td>0.79795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn_num_filters_20_pool_size_2_drop_0.5_l2_0.5...</td>\n",
       "      <td>0.328026</td>\n",
       "      <td>0.738538</td>\n",
       "      <td>0.89535</td>\n",
       "      <td>0.79760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           model_name  train_loss  dev_loss  \\\n",
       "32  cnn_num_filters_100_pool_size_1_drop_0.5_l2_1....    0.173732  0.973376   \n",
       "23  cnn_num_filters_10_pool_size_1_drop_0.5_l2_0.5...    0.418569  0.637613   \n",
       "31  cnn_num_filters_100_pool_size_1_drop_0.5_l2_0....    0.150977  0.993586   \n",
       "26  cnn_num_filters_50_pool_size_1_drop_0.5_l2_0.5...    0.201993  0.876053   \n",
       "24  cnn_num_filters_10_pool_size_1_drop_0.5_l2_1.0...    0.436353  0.634725   \n",
       "27  cnn_num_filters_50_pool_size_1_drop_0.5_l2_1.0...    0.227528  0.851900   \n",
       "30  cnn_num_filters_100_pool_size_1_drop_0.3_l2_0....    0.084707  1.127969   \n",
       "25  cnn_num_filters_50_pool_size_1_drop_0.3_l2_0.3...    0.122047  1.092002   \n",
       "18  cnn_num_filters_100_pool_size_2_drop_0.5_l2_0....    0.141580  1.062206   \n",
       "8   cnn_num_filters_20_pool_size_2_drop_0.5_l2_0.5...    0.328026  0.738538   \n",
       "\n",
       "    train_acc  dev_acc  \n",
       "32    0.96393  0.80485  \n",
       "23    0.86466  0.80480  \n",
       "31    0.96606  0.80470  \n",
       "26    0.94491  0.80460  \n",
       "24    0.86098  0.80265  \n",
       "27    0.94090  0.80250  \n",
       "30    0.98377  0.80100  \n",
       "25    0.97107  0.80080  \n",
       "18    0.96539  0.79795  \n",
       "8     0.89535  0.79760  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_metrics = metrics.sort_values(by=['dev_acc'], ascending=False)\n",
    "sorted_metrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_num_filters_100_pool_size_1_drop_0.5_l2_1.0_batch_size_512_epochs_100\n"
     ]
    }
   ],
   "source": [
    "model_name = list(sorted_metrics['model_name'])[0]\n",
    "print(model_name)\n",
    "best_model = load_model('./models/saved/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_test.csv')\n",
    "X_test = test_data['words'].values[:20000]\n",
    "y_test = test_data['entityType'].values[:20000]\n",
    "w2v_model = KeyedVectors.load('./models/google/word2vecGoogle.model')\n",
    "X_test = np.asarray(transform_input(X_test, w2v_model.vocab))\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_test = to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1s 65us/step\n"
     ]
    }
   ],
   "source": [
    "performance = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8748557825446129\n",
      "Test accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {}'.format(performance[0]))\n",
    "print('Test accuracy: {}'.format(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Depth Model for Sequence Tagging\n",
    "\n",
    "Ahora el input es cada oración y el output es una secuencia de etiquetas que asocia cada palabra de la oración con su respectiva etiqueta (PER - LOC - ORG - MISC - O). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entities</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['MISC', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O',...</td>\n",
       "      <td>['General', 'relativity', ',', 'or', 'the', 'g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['MISC', 'MISC', 'O', 'O', 'O', 'O', 'MISC', '...</td>\n",
       "      <td>['General', 'relativity', 'generalises', 'spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['In', 'particular', ',', 'the', 'curvature', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'MISC', 'MISC',...</td>\n",
       "      <td>['The', 'relation', 'is', 'specified', 'by', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['Some', 'predictions', 'of', 'general', 'rela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            entities  \\\n",
       "0  ['MISC', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O',...   \n",
       "1  ['MISC', 'MISC', 'O', 'O', 'O', 'O', 'MISC', '...   \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'MISC', 'MISC',...   \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "\n",
       "                                            sentence  \n",
       "0  ['General', 'relativity', ',', 'or', 'the', 'g...  \n",
       "1  ['General', 'relativity', 'generalises', 'spec...  \n",
       "2  ['In', 'particular', ',', 'the', 'curvature', ...  \n",
       "3  ['The', 'relation', 'is', 'specified', 'by', '...  \n",
       "4  ['Some', 'predictions', 'of', 'general', 'rela...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('../corpus_WiNER/seq_tag_instances/sen_entities_max_len_30_train.csv')\n",
    "data_val = pd.read_csv('../corpus_WiNER/seq_tag_instances/sen_entities_max_len_30_val.csv')\n",
    "data_test = pd.read_csv('../corpus_WiNER/seq_tag_instances/sen_entities_max_len_30_test.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['sentence'].map(lambda sen: np.array(ast.literal_eval(sen))).values\n",
    "y_train = data_train['entities'].map(lambda sen: np.array(ast.literal_eval(sen))).values\n",
    "X_val = data_val['sentence'].map(lambda sen: np.array(ast.literal_eval(sen))).values\n",
    "y_val = data_val['entities'].map(lambda sen: np.array(ast.literal_eval(sen))).values\n",
    "X_test = data_test['sentence'].map(lambda sen: np.array(ast.literal_eval(sen))).values\n",
    "y_test = data_test['entities'].map(lambda sen: np.array(ast.literal_eval(sen))).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 100000\n",
      "#val: 20000\n",
      "#test: 20000\n"
     ]
    }
   ],
   "source": [
    "print('#train:', len(X_train))\n",
    "print('#val:', len(X_val))\n",
    "print('#test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda y: tagToInt(y)\n",
    "y_train = np.array([np.array(list(map(fun, labels))) for labels in y_train])\n",
    "y_val = np.array([np.array(list(map(fun, labels))) for labels in y_val])\n",
    "y_test = np.array([np.array(list(map(fun, labels))) for labels in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "MAX_SEN_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in instance:\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return pad_sequences(word_indices, maxlen=MAX_SEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('../models/google/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)\n",
    "X_val = transform_input(X_val, w2v_model.vocab)\n",
    "X_test = transform_input(X_test, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1927,  78541,      0,     29,     11,    580,   5003,      0,\n",
       "        78541,      0,      4,     11,  49770,   5003,      0, 198897,\n",
       "         1377,     18,   8134,  27177,      1,      0,      0,     11,\n",
       "          403,   5852,      0, 198897,      1,   2360], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = np.array([to_categorical(labels, NUM_CLASSES) for labels in y_train])\n",
    "y_val = np.array([to_categorical(labels, NUM_CLASSES) for labels in y_val])\n",
    "y_test = np.array([to_categorical(labels, NUM_CLASSES) for labels in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "(100000, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(pad_sequences(y_train, maxlen=MAX_SEN_LEN, # [0,0,0,0,1] one-hot encoding for 'O' label\n",
    "                    value=np.array([0,0,0,0,1], dtype='float32')).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding target labels\n",
    "y_train = pad_sequences(y_train, maxlen=MAX_SEN_LEN, # [0,0,0,0,1] one-hot encoding for 'O' label\n",
    "                        value=np.array([0,0,0,0,1], dtype='float32'))\n",
    "y_val = pad_sequences(y_val, maxlen=MAX_SEN_LEN, # [0,0,0,0,1] one-hot encoding for 'O' label\n",
    "                      value=np.array([0,0,0,0,1], dtype='float32'))\n",
    "y_test = pad_sequences(y_test, maxlen=MAX_SEN_LEN, # [0,0,0,0,1] one-hot encoding for 'O' label\n",
    "                       value=np.array([0,0,0,0,1], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN Depth Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters = 100\n",
    "batch_size = 1024\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X_train.shape[1],))\n",
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=2, padding='same', activation='relu')(emb)\n",
    "# conv1_2 = Conv1D(filters=conv_filters, kernel_size=2, padding='same', activation='relu')(conv1_1)\n",
    "# conv1_3 = Conv1D(filters=conv_filters, kernel_size=2, padding='same', activation='relu')(conv1_2)\n",
    "drp1 = Dropout(0.5)(conv1_1)\n",
    "out = TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\"))(drp1)\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 30, 300)           900000000 \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 100)           60100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 5)             505       \n",
      "=================================================================\n",
      "Total params: 900,060,605\n",
      "Trainable params: 60,605\n",
      "Non-trainable params: 900,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 0.6494 - acc: 0.8783 - val_loss: 0.3253 - val_acc: 0.9033\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2727 - acc: 0.9210 - val_loss: 0.2606 - val_acc: 0.9215\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2418 - acc: 0.9294 - val_loss: 0.2474 - val_acc: 0.9260\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2318 - acc: 0.9326 - val_loss: 0.2412 - val_acc: 0.9290\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2264 - acc: 0.9343 - val_loss: 0.2375 - val_acc: 0.9303\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2228 - acc: 0.9355 - val_loss: 0.2348 - val_acc: 0.9312\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2205 - acc: 0.9363 - val_loss: 0.2329 - val_acc: 0.9318\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2185 - acc: 0.9370 - val_loss: 0.2314 - val_acc: 0.9325\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2168 - acc: 0.9375 - val_loss: 0.2302 - val_acc: 0.9328\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2156 - acc: 0.9379 - val_loss: 0.2295 - val_acc: 0.9331\n",
      "Epoch 11/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2144 - acc: 0.9383 - val_loss: 0.2286 - val_acc: 0.9334\n",
      "Epoch 12/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2133 - acc: 0.9385 - val_loss: 0.2276 - val_acc: 0.9336\n",
      "Epoch 13/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2123 - acc: 0.9388 - val_loss: 0.2270 - val_acc: 0.9338\n",
      "Epoch 14/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2115 - acc: 0.9390 - val_loss: 0.2265 - val_acc: 0.9339\n",
      "Epoch 15/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2108 - acc: 0.9392 - val_loss: 0.2260 - val_acc: 0.9340\n",
      "Epoch 16/100\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.2103 - acc: 0.9393 - val_loss: 0.2255 - val_acc: 0.9341\n",
      "Epoch 17/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2096 - acc: 0.9395 - val_loss: 0.2252 - val_acc: 0.9342\n",
      "Epoch 18/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2092 - acc: 0.9397 - val_loss: 0.2247 - val_acc: 0.9343\n",
      "Epoch 19/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2087 - acc: 0.9397 - val_loss: 0.2245 - val_acc: 0.9344\n",
      "Epoch 20/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2081 - acc: 0.9399 - val_loss: 0.2241 - val_acc: 0.9345\n",
      "Epoch 21/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2077 - acc: 0.9400 - val_loss: 0.2238 - val_acc: 0.9346\n",
      "Epoch 22/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2072 - acc: 0.9401 - val_loss: 0.2236 - val_acc: 0.9347\n",
      "Epoch 23/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2068 - acc: 0.9400 - val_loss: 0.2232 - val_acc: 0.9349\n",
      "Epoch 24/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2066 - acc: 0.9403 - val_loss: 0.2232 - val_acc: 0.9347\n",
      "Epoch 25/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2063 - acc: 0.9403 - val_loss: 0.2229 - val_acc: 0.9350\n",
      "Epoch 26/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2060 - acc: 0.9404 - val_loss: 0.2226 - val_acc: 0.9351\n",
      "Epoch 27/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.2056 - acc: 0.9405 - val_loss: 0.2223 - val_acc: 0.9350\n",
      "Epoch 28/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2051 - acc: 0.9406 - val_loss: 0.2222 - val_acc: 0.9351\n",
      "Epoch 29/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2049 - acc: 0.9407 - val_loss: 0.2218 - val_acc: 0.9352\n",
      "Epoch 30/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2046 - acc: 0.9406 - val_loss: 0.2217 - val_acc: 0.9351\n",
      "Epoch 31/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2042 - acc: 0.9408 - val_loss: 0.2215 - val_acc: 0.9351\n",
      "Epoch 32/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2041 - acc: 0.9408 - val_loss: 0.2213 - val_acc: 0.9353\n",
      "Epoch 33/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2037 - acc: 0.9409 - val_loss: 0.2212 - val_acc: 0.9353\n",
      "Epoch 34/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2036 - acc: 0.9410 - val_loss: 0.2211 - val_acc: 0.9352\n",
      "Epoch 35/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2034 - acc: 0.9409 - val_loss: 0.2209 - val_acc: 0.9353\n",
      "Epoch 36/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2033 - acc: 0.9411 - val_loss: 0.2208 - val_acc: 0.9354\n",
      "Epoch 37/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2030 - acc: 0.9412 - val_loss: 0.2206 - val_acc: 0.9354\n",
      "Epoch 38/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2029 - acc: 0.9412 - val_loss: 0.2206 - val_acc: 0.9354\n",
      "Epoch 39/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2026 - acc: 0.9412 - val_loss: 0.2205 - val_acc: 0.9354\n",
      "Epoch 40/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2024 - acc: 0.9412 - val_loss: 0.2203 - val_acc: 0.9354\n",
      "Epoch 41/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2021 - acc: 0.9414 - val_loss: 0.2202 - val_acc: 0.9355\n",
      "Epoch 42/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2021 - acc: 0.9414 - val_loss: 0.2200 - val_acc: 0.9355\n",
      "Epoch 43/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2019 - acc: 0.9415 - val_loss: 0.2199 - val_acc: 0.9355\n",
      "Epoch 44/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2017 - acc: 0.9414 - val_loss: 0.2198 - val_acc: 0.9356\n",
      "Epoch 45/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2016 - acc: 0.9416 - val_loss: 0.2199 - val_acc: 0.9355\n",
      "Epoch 46/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2013 - acc: 0.9416 - val_loss: 0.2196 - val_acc: 0.9356\n",
      "Epoch 47/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2011 - acc: 0.9416 - val_loss: 0.2195 - val_acc: 0.9356\n",
      "Epoch 48/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.2010 - acc: 0.9417 - val_loss: 0.2195 - val_acc: 0.9357\n",
      "Epoch 49/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2009 - acc: 0.9417 - val_loss: 0.2194 - val_acc: 0.9357\n",
      "Epoch 50/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2008 - acc: 0.9417 - val_loss: 0.2193 - val_acc: 0.9357\n",
      "Epoch 51/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2007 - acc: 0.9418 - val_loss: 0.2192 - val_acc: 0.9358\n",
      "Epoch 52/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.2004 - acc: 0.9418 - val_loss: 0.2192 - val_acc: 0.9357\n",
      "Epoch 53/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2002 - acc: 0.9419 - val_loss: 0.2189 - val_acc: 0.9358\n",
      "Epoch 54/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2003 - acc: 0.9419 - val_loss: 0.2190 - val_acc: 0.9357\n",
      "Epoch 55/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.2000 - acc: 0.9419 - val_loss: 0.2188 - val_acc: 0.9357\n",
      "Epoch 56/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1999 - acc: 0.9419 - val_loss: 0.2190 - val_acc: 0.9358\n",
      "Epoch 57/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1998 - acc: 0.9419 - val_loss: 0.2187 - val_acc: 0.9359\n",
      "Epoch 58/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1997 - acc: 0.9420 - val_loss: 0.2187 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1996 - acc: 0.9420 - val_loss: 0.2185 - val_acc: 0.9359\n",
      "Epoch 60/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1995 - acc: 0.9420 - val_loss: 0.2183 - val_acc: 0.9359\n",
      "Epoch 61/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1994 - acc: 0.9421 - val_loss: 0.2184 - val_acc: 0.9360\n",
      "Epoch 62/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1991 - acc: 0.9422 - val_loss: 0.2184 - val_acc: 0.9359\n",
      "Epoch 63/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1991 - acc: 0.9422 - val_loss: 0.2182 - val_acc: 0.9359\n",
      "Epoch 64/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1989 - acc: 0.9423 - val_loss: 0.2181 - val_acc: 0.9360\n",
      "Epoch 65/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1989 - acc: 0.9422 - val_loss: 0.2182 - val_acc: 0.9360\n",
      "Epoch 66/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1988 - acc: 0.9422 - val_loss: 0.2180 - val_acc: 0.9360\n",
      "Epoch 67/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1986 - acc: 0.9423 - val_loss: 0.2178 - val_acc: 0.9361\n",
      "Epoch 68/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1986 - acc: 0.9424 - val_loss: 0.2178 - val_acc: 0.9360\n",
      "Epoch 69/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1984 - acc: 0.9424 - val_loss: 0.2177 - val_acc: 0.9360\n",
      "Epoch 70/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1983 - acc: 0.9424 - val_loss: 0.2176 - val_acc: 0.9360\n",
      "Epoch 71/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1983 - acc: 0.9423 - val_loss: 0.2178 - val_acc: 0.9359\n",
      "Epoch 72/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1982 - acc: 0.9424 - val_loss: 0.2175 - val_acc: 0.9360\n",
      "Epoch 73/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1982 - acc: 0.9424 - val_loss: 0.2176 - val_acc: 0.9361\n",
      "Epoch 74/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1980 - acc: 0.9426 - val_loss: 0.2176 - val_acc: 0.9361\n",
      "Epoch 75/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.1980 - acc: 0.9426 - val_loss: 0.2175 - val_acc: 0.9361\n",
      "Epoch 76/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1978 - acc: 0.9426 - val_loss: 0.2173 - val_acc: 0.9362\n",
      "Epoch 77/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1978 - acc: 0.9425 - val_loss: 0.2174 - val_acc: 0.9362\n",
      "Epoch 78/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1977 - acc: 0.9426 - val_loss: 0.2173 - val_acc: 0.9362\n",
      "Epoch 79/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1975 - acc: 0.9426 - val_loss: 0.2173 - val_acc: 0.9362\n",
      "Epoch 80/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1974 - acc: 0.9427 - val_loss: 0.2174 - val_acc: 0.9363\n",
      "Epoch 81/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1974 - acc: 0.9427 - val_loss: 0.2173 - val_acc: 0.9362\n",
      "Epoch 82/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1974 - acc: 0.9426 - val_loss: 0.2171 - val_acc: 0.9363\n",
      "Epoch 83/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1971 - acc: 0.9427 - val_loss: 0.2171 - val_acc: 0.9362\n",
      "Epoch 84/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1971 - acc: 0.9427 - val_loss: 0.2170 - val_acc: 0.9364\n",
      "Epoch 85/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1972 - acc: 0.9427 - val_loss: 0.2170 - val_acc: 0.9364\n",
      "Epoch 86/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1969 - acc: 0.9428 - val_loss: 0.2169 - val_acc: 0.9364\n",
      "Epoch 87/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1968 - acc: 0.9428 - val_loss: 0.2168 - val_acc: 0.9364\n",
      "Epoch 88/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1969 - acc: 0.9428 - val_loss: 0.2169 - val_acc: 0.9364\n",
      "Epoch 89/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1968 - acc: 0.9428 - val_loss: 0.2167 - val_acc: 0.9365\n",
      "Epoch 90/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1967 - acc: 0.9428 - val_loss: 0.2166 - val_acc: 0.9365\n",
      "Epoch 91/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1966 - acc: 0.9429 - val_loss: 0.2167 - val_acc: 0.9365\n",
      "Epoch 92/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1967 - acc: 0.9428 - val_loss: 0.2166 - val_acc: 0.9365\n",
      "Epoch 93/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1965 - acc: 0.9429 - val_loss: 0.2166 - val_acc: 0.9365\n",
      "Epoch 94/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1964 - acc: 0.9430 - val_loss: 0.2165 - val_acc: 0.9365\n",
      "Epoch 95/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.1964 - acc: 0.9429 - val_loss: 0.2164 - val_acc: 0.9367\n",
      "Epoch 96/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1963 - acc: 0.9430 - val_loss: 0.2164 - val_acc: 0.9365\n",
      "Epoch 97/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.1961 - acc: 0.9430 - val_loss: 0.2163 - val_acc: 0.9366\n",
      "Epoch 98/100\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.1961 - acc: 0.9430 - val_loss: 0.2163 - val_acc: 0.9366\n",
      "Epoch 99/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1959 - acc: 0.9431 - val_loss: 0.2162 - val_acc: 0.9367\n",
      "Epoch 100/100\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.1961 - acc: 0.9430 - val_loss: 0.2165 - val_acc: 0.9366\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1s 42us/step\n"
     ]
    }
   ],
   "source": [
    "performance = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2073960896730423, 0.9378216666221618]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
