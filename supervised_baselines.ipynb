{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Lambda, Embedding\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from corpus_WiNER.corpus_utils import *\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP \n",
    "\n",
    "Utilizando la estrategia de decaimiento exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_train_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_train_exp_decay_W_5.npz')\n",
    "X_train = word_vectors.items()[0][1][:100000]\n",
    "y_train = entity_vector.items()[0][1][:100000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_dev_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_dev_exp_decay_W_5.npz')\n",
    "X_dev = word_vectors.items()[0][1][:20000]\n",
    "y_dev = entity_vector.items()[0][1][:20000]\n",
    "word_vectors = np.load('./corpus_WiNER/word_vectors/wv_test_exp_decay_W_5.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_test_exp_decay_W_5.npz')\n",
    "X_test = word_vectors.items()[0][1][:20000]\n",
    "y_test = entity_vector.items()[0][1][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word vectors: 100000\n",
      "# non entities 55825\n",
      "# word vectors: 20000\n",
      "# non entities 9634\n",
      "# word vectors: 20000\n",
      "# non entities 10410\n"
     ]
    }
   ],
   "source": [
    "print('# word vectors:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# word vectors:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# word vectors:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 2, 2, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Función que genera los modelos y que se usara en la grilla de validación cruzada.\n",
    "# def build_model2(nodes1 = 100, nodes2 = 200, lr = 0.001, \n",
    "#                 l2 = 0.01, drop = 0.1):\n",
    "nodes1 = 300\n",
    "nodes2 = 512\n",
    "lr = 0.001\n",
    "l2 = 0.01\n",
    "drop = 0.1\n",
    "\n",
    "model = Sequential()    \n",
    "model.add(Dense(nodes1,\n",
    "                input_shape=(300,),\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)\n",
    "         )\n",
    ")      \n",
    "model.add(Dropout(drop))   \n",
    "\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(nodes2,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(256,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "\n",
    "\n",
    "model.add(Dropout(drop))\n",
    "model.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer = regularizers.l2(l2)))\n",
    "model.add(Dense(5, activation = 'softmax')) # PER - LOC - ORG - MISC - O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 737,729\n",
      "Trainable params: 737,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adadelta(lr = lr),\n",
    "              loss = categorical_crossentropy,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 4s 43us/step - loss: 21.1002 - acc: 0.1901 - val_loss: 21.0348 - val_acc: 0.3146\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.9520 - acc: 0.4330 - val_loss: 20.8860 - val_acc: 0.4757\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 3s 35us/step - loss: 20.7924 - acc: 0.5508 - val_loss: 20.7286 - val_acc: 0.4817\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.6237 - acc: 0.5582 - val_loss: 20.5644 - val_acc: 0.4817\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 3s 33us/step - loss: 20.4466 - acc: 0.5582 - val_loss: 20.3952 - val_acc: 0.4817\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.2633 - acc: 0.5582 - val_loss: 20.2245 - val_acc: 0.4817\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 20.0786 - acc: 0.5583 - val_loss: 20.0587 - val_acc: 0.4817\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.9006 - acc: 0.5582 - val_loss: 19.9039 - val_acc: 0.4817\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.7362 - acc: 0.5583 - val_loss: 19.7597 - val_acc: 0.4817\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 3s 34us/step - loss: 19.5839 - acc: 0.5583 - val_loss: 19.6206 - val_acc: 0.4817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN \n",
    "\n",
    "Utilizando ventana simétrica de palabras que rodea a la objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos pre-procesados y los filtramos para obtener:\n",
    "\n",
    "- 100000 instancias de train\n",
    "- &nbsp; 20000 instancias de dev\n",
    "- &nbsp; 20000 instancias de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['', '', 'Watching', 'Ellie', 'is']</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['', 'Watching', 'Ellie', 'is', 'an']</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Watching', 'Ellie', 'is', 'an', 'American']</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['is', 'an', 'American', 'sitcom', 'that']</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['that', 'starred', 'Julia', 'Louis-Dreyfus', ...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words entityType\n",
       "0                ['', '', 'Watching', 'Ellie', 'is']       MISC\n",
       "1              ['', 'Watching', 'Ellie', 'is', 'an']       MISC\n",
       "2      ['Watching', 'Ellie', 'is', 'an', 'American']          O\n",
       "3         ['is', 'an', 'American', 'sitcom', 'that']        LOC\n",
       "4  ['that', 'starred', 'Julia', 'Louis-Dreyfus', ...        PER"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_train.csv')\n",
    "data_dev = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_dev.csv')\n",
    "data_test = pd.read_csv('./corpus_WiNER/cnn_instances/words_entity_W_2_cnn_test.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['words'].values[:100000]\n",
    "y_train = data_train['entityType'].values[:100000]\n",
    "X_dev = data_dev['words'].values[:20000]\n",
    "y_dev = data_dev['entityType'].values[:20000]\n",
    "X_test = data_test['words'].values[:20000]\n",
    "y_test = data_test['entityType'].values[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances: 100000\n",
      "# non entities 54508\n",
      "# instances: 20000\n",
      "# non entities 9590\n",
      "# instances: 20000\n",
      "# non entities 10856\n"
     ]
    }
   ],
   "source": [
    "print('# instances:', len(X_train))\n",
    "print('# non entities', list(y_train).count('O'))\n",
    "print('# instances:', len(X_dev))\n",
    "print('# non entities', list(y_dev).count('O'))\n",
    "print('# instances:', len(X_test))\n",
    "print('# non entities', list(y_test).count('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 1, 0, 0, 0, 0, 4, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_dev = [tagToInt(y) for y in y_dev]\n",
    "y_test = [tagToInt(y) for y in y_test]\n",
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in ast.literal_eval(instance):\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('./models/google/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)\n",
    "X_dev = transform_input(X_dev, w2v_model.vocab)\n",
    "X_test = transform_input(X_test, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # For mini-batch gradient descent\n",
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "epochs = 10\n",
    "len_words = 5\n",
    "input_size = len_words # amount of words by row\n",
    "train_examples = len(X_train)\n",
    "test_examples = len(X_test)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_dev = to_categorical(y_dev, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d\n",
    "steps = 5 # number of words in the sentence\n",
    "channels = 1\n",
    "input_shape = (steps, channels) #3D tensor with shape: `(batch, steps, channels)`\n",
    "# # Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "X_train = np.asarray(X_train)\n",
    "X_dev = np.asarray(X_dev)\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters = 10\n",
    "pool_size = 2\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)\n",
    "print(emb.shape)\n",
    "# The output here has shape (batch_size (?), words_in_reviews (?), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "#     3D tensor with shape: `(batch, steps, channels)`\n",
    "\n",
    "# Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "#     `steps` value might have changed due to padding or strides.\n",
    "\n",
    "# Specify each convolution layer and their kernel size i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=2, activation='relu')(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "maxp1_1 = MaxPooling1D(pool_size=pool_size)(btch1_1)\n",
    "flat1_1 = Flatten()(maxp1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "maxp1_2 = MaxPooling1D(pool_size=pool_size)(btch1_2)\n",
    "flat1_2 = Flatten()(maxp1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "maxp1_3 = MaxPooling1D(pool_size=pool_size)(btch1_3)\n",
    "flat1_3 = Flatten()(maxp1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([flat1_1, flat1_2, flat1_3], axis=1)\n",
    "drp1 = Dropout(0)(cnct)\n",
    "\n",
    "dns1  = Dense(128, activation='relu')(drp1)\n",
    "out = Dense(num_classes, activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
