{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, concatenate, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Activation, Embedding\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityListFromSentence(senIdx, sen_length, art_entities_df):\n",
    "    # We take the df with the entities of each sentence\n",
    "    sen_entities_df = art_entities_df[art_entities_df.senIdx == senIdx]\n",
    "    # An empty dataframe means that the sentence doesn't have any entity\n",
    "    if sen_entities_df.empty:\n",
    "        entities = ['O' for _ in range(sen_length)]\n",
    "    else:\n",
    "        entities = []\n",
    "        i = 0\n",
    "        for _, row in sen_entities_df.iterrows():\n",
    "            while i < row['begin']:\n",
    "                entities.append('O')\n",
    "                i += 1\n",
    "            while i < row['end']:\n",
    "                entities.append(row['entityType'])\n",
    "                i += 1\n",
    "        while i < sen_length:\n",
    "            entities.append('O')\n",
    "            i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los primeros 1500 documentos que se van a utilizar para la parte supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "doc_filenames.sort()\n",
    "doc_filenames = doc_filenames[0:1500]\n",
    "coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "coarseNE_filenames.sort()\n",
    "coarseNE_filenames = coarseNE_filenames[0:1500]\n",
    "docs = []\n",
    "coarseNEs = []\n",
    "for doc, ne in zip(doc_filenames, coarseNE_filenames):\n",
    "    docs.append(pd.read_pickle('./corpus_WiNER/docs_df/'+ doc))\n",
    "    coarseNEs.append(pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ ne))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.concat(docs, ignore_index=True)\n",
    "coarseNE_df = pd.concat(coarseNEs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de oraciones:', docs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de artículos que no contienen ninguna entidad:', \n",
    "      len(docs_df.art_ID.unique()) - len(coarseNE_df.art_ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con aquellos artículos que contienen al menos una entidad nombrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_IDs = coarseNE_df.art_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos una muestra aleatoria de 4000 artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(art_IDs)\n",
    "art_IDs_sample = art_IDs[0:4000]\n",
    "#### Filtramos\n",
    "articles_df = docs_df[docs_df.art_ID.isin(art_IDs_sample)]\n",
    "entities_df = coarseNE_df[coarseNE_df.art_ID.isin(art_IDs_sample)]\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider only the articles with at least one entity.\n",
    "# That's why we iterate over the coarseNE's articles.\n",
    "sentences = []\n",
    "entities = []\n",
    "for art_ID in tqdm(np.nditer(art_IDs_sample)):\n",
    "    article_df = articles_df[articles_df.art_ID == art_ID]\n",
    "    art_entities_df = entities_df[entities_df.art_ID == art_ID] \n",
    "    article_df = article_df.reset_index(drop=True) # this is important for the entity matching.\n",
    "    article_df['sen_length'] = article_df['sentence'].map(lambda x: len(x))\n",
    "    fun = lambda senIdx: entityListFromSentence(senIdx, article_df.loc[senIdx, 'sen_length'],\n",
    "                                                 art_entities_df)    \n",
    "    article_df['entities'] = article_df.index.map(fun)\n",
    "    sentences += list(article_df['sentence'])\n",
    "    entities += list(article_df['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'sentence':sentences, 'entities':entities})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spread_words_entities(sentence, sen_entities, W):\n",
    "    new_input = []\n",
    "    L = len(sentence)\n",
    "    # I: index of the target word\n",
    "    for I in range(0, L):             \n",
    "        words = []      \n",
    "        # Padding with zeros on the left\n",
    "        if I - W < 0:\n",
    "            words += [''] * abs(I-W) #list(np.zeros(abs(I-W), dtype=int))\n",
    "        # Concat vectors from the sentence\n",
    "        for i in range(I - W, I + W + 1):             \n",
    "            if i >= 0 and i < L:\n",
    "                words.append(sentence[i])      \n",
    "        # Padding with vector of zeros on the right\n",
    "        if I + W >= L:\n",
    "            words += list(np.zeros(abs(I+W+1-L), dtype=int))\n",
    "        new_input.append((words, sen_entities[I]))\n",
    "        \n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = []\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    new_input += spread_words_entities(row['sentence'], row['entities'], W=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame(new_input, columns=['words', 'entityType'])\n",
    "print(input_data.shape)\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_entities(df, frac):\n",
    "    '''\n",
    "    Remove a fraction of non entities vectors (entityType == 'O')\n",
    "    df: wordVector_Entity_df\n",
    "    frac: float value between 0 and 1\n",
    "    @return df with a fraction of the non entities rows removed\n",
    "    '''\n",
    "    sample = df[df.entityType == 'O'].sample(frac=frac, random_state=77)\n",
    "    return df.drop(index=sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = drop_non_entities(input_data, 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_csv('./corpus_WiNER/words_entity_cnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos ya pre-procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518696, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O       273639\n",
       "MISC     91899\n",
       "LOC      58713\n",
       "PER      57497\n",
       "ORG      36948\n",
       "Name: entityType, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.read_csv('./corpus_WiNER/words_entity_cnn_test.csv')\n",
    "print(input_data.shape)\n",
    "input_data['entityType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['', '', 'Pier', 'Francesco', \"d'Jacopo\"]</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['', 'Pier', 'Francesco', \"d'Jacopo\", 'di']</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Pier', 'Francesco', \"d'Jacopo\", 'di', 'Domen...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Francesco', \"d'Jacopo\", 'di', 'Domenico', 'T...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"d'Jacopo\", 'di', 'Domenico', 'Toschi', '-LRB-']</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words entityType\n",
       "0          ['', '', 'Pier', 'Francesco', \"d'Jacopo\"]        PER\n",
       "1        ['', 'Pier', 'Francesco', \"d'Jacopo\", 'di']        PER\n",
       "2  ['Pier', 'Francesco', \"d'Jacopo\", 'di', 'Domen...        PER\n",
       "3  ['Francesco', \"d'Jacopo\", 'di', 'Domenico', 'T...        PER\n",
       "4  [\"d'Jacopo\", 'di', 'Domenico', 'Toschi', '-LRB-']        PER"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos los datos en train - dev - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 466826\n",
      "#test: 51870\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_data['words'], input_data['entityType'],\n",
    "                                                    test_size=0.10, \n",
    "                                                    random_state=42)\n",
    "print('#train:', len(X_train))\n",
    "print('#test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagToInt(tag):\n",
    "    return {'O': 0, 'PER': 1, 'ORG': 2, 'LOC': 3, 'MISC': 4}[tag]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_test = [tagToInt(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 4, 0, 0, 0, 0, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in ast.literal_eval(instance):\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # For mini-batch gradient descent\n",
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "epochs = 10\n",
    "len_words = 5\n",
    "input_size = len_words # amount of words by row\n",
    "train_examples = len(X_train)\n",
    "test_examples = len(X_test)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d\n",
    "steps = 5 # number of words in the sentence\n",
    "channels = 1\n",
    "input_shape = (steps, channels) #3D tensor with shape: `(batch, steps, channels)`\n",
    "# # Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466826, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,  66867,  46891,     10],\n",
       "       [   499,     11,   5221,   1393,      0],\n",
       "       [     0,  36989, 246915,      0,     26],\n",
       "       [   166,   1187,     19,    379,      2],\n",
       "       [    11,   9867,    111,      0, 159050]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 10\n",
    "pool_size = 2\n",
    "inp = Input(shape=(X_train.shape[1], 1))\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)\n",
    "print(emb.shape)\n",
    "# The output here has shape (batch_size (?), words_in_reviews (?), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 300)\n"
     ]
    }
   ],
   "source": [
    "# lamb = Lambda(lambda xin: K.concatenate(xin, axis=1), name='embedding_concat')(emb)\n",
    "# print(lamb.shape)\n",
    "emb = K.squeeze(emb, axis=2)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_inbound_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-09e2af0742cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdns1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 237\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m                   tensor_index=tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1340\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis_env/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdetected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \"\"\"\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;31m# Prevent cycles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_inbound_nodes'"
     ]
    }
   ],
   "source": [
    "# Specify each convolution layer and their kernel size i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "maxp1_1 = MaxPooling1D(pool_size=pool_size)(btch1_1)\n",
    "flat1_1 = Flatten()(maxp1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "maxp1_2 = MaxPooling1D(pool_size=pool_size)(btch1_2)\n",
    "flat1_2 = Flatten()(maxp1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "maxp1_3 = MaxPooling1D(pool_size=pool_size)(btch1_3)\n",
    "flat1_3 = Flatten()(maxp1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([flat1_1, flat1_2, flat1_3], axis=1)\n",
    "drp1 = Dropout(0)(cnct)\n",
    "\n",
    "dns1  = Dense(128, activation='relu')(drp1)\n",
    "out = Dense(num_classes, activation='softmax')(dns1)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(X_test, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backup: sino lo uso borrar del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = pd.read_csv('./corpus_WiNER/document.vocab', sep=' ', header=None, \n",
    "                           names=['word', 'frequency'], keep_default_na=False) \n",
    "                        # con esto evito que el parser trate como nan value a algunas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_mapping.shape)\n",
    "word_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_mapping['word'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los artículos del Documento \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = pd.read_csv('./corpus_WiNER/Documents/0', sep='ID', engine='python', header=None, names=['sentence', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a asociarle a cada oración el ID del artículo al cual pertenece.\n",
    "\n",
    "Es importante recordar que el orden de las oraciones está dado por los índices del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import spread_artID\n",
    "doc_0 = spread_artID(doc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos ahora las filas con NaN que contenian los ID de los artículos inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = doc_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El documento contiene {} oraciones.'.format(doc_0.shape[0]))\n",
    "print('El documento contiene {} artículos'.format(len(doc_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos que cada sentencia sea una lista de palabras codificadas y casteamos a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0['sentence'] = doc_0['sentence'].map(lambda x: list(map(int, x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con un artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = doc_0[doc_0.art_ID == 1000]\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruimos la primera oración del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_decoder(sentence, word_mapping):\n",
    "    dec_sentence = []\n",
    "    for idx in sentence:\n",
    "        mapped_w = word_mapping.loc[idx, 'word']\n",
    "        dec_sentence.append(mapped_w)\n",
    "    return dec_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos article.columns.get_loc('sentence') para evitar hardcodear el índice correspondiente\n",
    "# a la columna 'sentence' que en este caso es 0\n",
    "sentence = article_df.iloc[0, article_df.columns.get_loc('sentence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_0 = article_df.sentence[0]\n",
    "print(sen_0)\n",
    "sen_1 = article_df.sentence[1]\n",
    "print(sen_1)\n",
    "sen_2 = article_df.sentence[2]\n",
    "print(sen_2)\n",
    "\n",
    "l = []\n",
    "l.append(sen_0)\n",
    "l.append(sen_1)\n",
    "l.append(sen_2)\n",
    "\n",
    "pad_sequences(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
