{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, concatenate, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Activation, Embedding\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityListFromSentence(senIdx, sen_length, art_entities_df):\n",
    "    # We take the df with the entities of each sentence\n",
    "    sen_entities_df = art_entities_df[art_entities_df.senIdx == senIdx]\n",
    "    # An empty dataframe means that the sentence doesn't have any entity\n",
    "    if sen_entities_df.empty:\n",
    "        entities = ['O' for _ in range(sen_length)]\n",
    "    else:\n",
    "        entities = []\n",
    "        i = 0\n",
    "        for _, row in sen_entities_df.iterrows():\n",
    "            while i < row['begin']:\n",
    "                entities.append('O')\n",
    "                i += 1\n",
    "            while i < row['end']:\n",
    "                entities.append(row['entityType'])\n",
    "                i += 1\n",
    "        while i < sen_length:\n",
    "            entities.append('O')\n",
    "            i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los primeros 1500 documentos que se van a utilizar para la parte supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "doc_filenames.sort()\n",
    "doc_filenames = doc_filenames[0:1500]\n",
    "coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "coarseNE_filenames.sort()\n",
    "coarseNE_filenames = coarseNE_filenames[0:1500]\n",
    "docs = []\n",
    "coarseNEs = []\n",
    "for doc, ne in zip(doc_filenames, coarseNE_filenames):\n",
    "    docs.append(pd.read_pickle('./corpus_WiNER/docs_df/'+ doc))\n",
    "    coarseNEs.append(pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ ne))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.concat(docs, ignore_index=True)\n",
    "coarseNE_df = pd.concat(coarseNEs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de oraciones:', docs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de artículos que no contienen ninguna entidad:', \n",
    "      len(docs_df.art_ID.unique()) - len(coarseNE_df.art_ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con aquellos artículos que contienen al menos una entidad nombrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_IDs = coarseNE_df.art_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos una muestra aleatoria de 4000 artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(art_IDs)\n",
    "art_IDs_sample = art_IDs[0:4000]\n",
    "#### Filtramos\n",
    "articles_df = docs_df[docs_df.art_ID.isin(art_IDs_sample)]\n",
    "entities_df = coarseNE_df[coarseNE_df.art_ID.isin(art_IDs_sample)]\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider only the articles with at least one entity.\n",
    "# That's why we iterate over the coarseNE's articles.\n",
    "sentences = []\n",
    "entities = []\n",
    "for art_ID in tqdm(np.nditer(art_IDs_sample)):\n",
    "    article_df = articles_df[articles_df.art_ID == art_ID]\n",
    "    art_entities_df = entities_df[entities_df.art_ID == art_ID] \n",
    "    article_df = article_df.reset_index(drop=True) # this is important for the entity matching.\n",
    "    article_df['sen_length'] = article_df['sentence'].map(lambda x: len(x))\n",
    "    fun = lambda senIdx: entityListFromSentence(senIdx, article_df.loc[senIdx, 'sen_length'],\n",
    "                                                 art_entities_df)    \n",
    "    article_df['entities'] = article_df.index.map(fun)\n",
    "    sentences += list(article_df['sentence'])\n",
    "    entities += list(article_df['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'sentence':sentences, 'entities':entities})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spread_words_entities(sentence, sen_entities, W):\n",
    "    new_input = []\n",
    "    L = len(sentence)\n",
    "    # I: index of the target word\n",
    "    for I in range(0, L):             \n",
    "        words = []      \n",
    "        # Padding with zeros on the left\n",
    "        if I - W < 0:\n",
    "            words += [''] * abs(I-W) #list(np.zeros(abs(I-W), dtype=int))\n",
    "        # Concat vectors from the sentence\n",
    "        for i in range(I - W, I + W + 1):             \n",
    "            if i >= 0 and i < L:\n",
    "                words.append(sentence[i])      \n",
    "        # Padding with vector of zeros on the right\n",
    "        if I + W >= L:\n",
    "            words += list(np.zeros(abs(I+W+1-L), dtype=int))\n",
    "        new_input.append((words, sen_entities[I]))\n",
    "        \n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = []\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    new_input += spread_words_entities(row['sentence'], row['entities'], W=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame(new_input, columns=['words', 'entityType'])\n",
    "print(input_data.shape)\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_entities(df, frac):\n",
    "    '''\n",
    "    Remove a fraction of non entities vectors (entityType == 'O')\n",
    "    df: wordVector_Entity_df\n",
    "    frac: float value between 0 and 1\n",
    "    @return df with a fraction of the non entities rows removed\n",
    "    '''\n",
    "    sample = df[df.entityType == 'O'].sample(frac=frac, random_state=77)\n",
    "    return df.drop(index=sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = drop_non_entities(input_data, 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_csv('./corpus_WiNER/words_entity_cnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos ya pre-procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518696, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O       273639\n",
       "MISC     91899\n",
       "LOC      58713\n",
       "PER      57497\n",
       "ORG      36948\n",
       "Name: entityType, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.read_csv('./corpus_WiNER/words_entity_cnn_test.csv')\n",
    "print(input_data.shape)\n",
    "input_data['entityType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['', '', 'Pier', 'Francesco', \"d'Jacopo\"]</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['', 'Pier', 'Francesco', \"d'Jacopo\", 'di']</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Pier', 'Francesco', \"d'Jacopo\", 'di', 'Domen...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Francesco', \"d'Jacopo\", 'di', 'Domenico', 'T...</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"d'Jacopo\", 'di', 'Domenico', 'Toschi', '-LRB-']</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words entityType\n",
       "0          ['', '', 'Pier', 'Francesco', \"d'Jacopo\"]        PER\n",
       "1        ['', 'Pier', 'Francesco', \"d'Jacopo\", 'di']        PER\n",
       "2  ['Pier', 'Francesco', \"d'Jacopo\", 'di', 'Domen...        PER\n",
       "3  ['Francesco', \"d'Jacopo\", 'di', 'Domenico', 'T...        PER\n",
       "4  [\"d'Jacopo\", 'di', 'Domenico', 'Toschi', '-LRB-']        PER"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos los datos en train - dev - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 466826\n",
      "#test: 51870\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_data['words'], input_data['entityType'],\n",
    "                                                    test_size=0.10, \n",
    "                                                    random_state=42)\n",
    "print('#train:', len(X_train))\n",
    "print('#test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagToInt(tag):\n",
    "    return {'O': 0, 'PER': 1, 'ORG': 2, 'LOC': 3, 'MISC': 4}[tag]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [tagToInt(y) for y in y_train]\n",
    "y_test = [tagToInt(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 4, 0, 0, 0, 0, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10] # this transformation is needed to apply to_categorical() keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, m_words).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        l = []\n",
    "        for word in ast.literal_eval(instance):\n",
    "            try:\n",
    "                l.append(mapping[word].index)\n",
    "            except KeyError:\n",
    "                l.append(0) # index to '</s>' word vector\n",
    "        word_indices.append(l)\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_input(X_train, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # For mini-batch gradient descent\n",
    "num_classes = 5 # PER - LOC - ORG - MISC - O\n",
    "epochs = 10\n",
    "len_words = 5\n",
    "input_size = len_words # amount of words by row\n",
    "train_examples = len(X_train)\n",
    "test_examples = len(X_test)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d\n",
    "steps = 5 # number of words in the sentence\n",
    "channels = 1\n",
    "input_shape = (steps, channels) #3D tensor with shape: `(batch, steps, channels)`\n",
    "# # Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466826, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,  66867,  46891,     10],\n",
       "       [   499,     11,   5221,   1393,      0],\n",
       "       [     0,  36989, 246915,      0,     26],\n",
       "       [   166,   1187,     19,    379,      2],\n",
       "       [    11,   9867,    111,      0, 159050]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5)\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 10\n",
    "pool_size = 2\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 300)\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(w2v_model.vocab),  # Vocabulary size\n",
    "                w2v_model.vector_size, # Embedding size\n",
    "                weights=[w2v_model.vectors], # Word vectors\n",
    "                trainable=False  # This indicates the word vectors must not be changed\n",
    "                                 # during training.\n",
    "      )(inp)\n",
    "print(emb.shape)\n",
    "# The output here has shape (batch_size (?), words_in_reviews (?), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamb = Lambda(lambda xin: K.concatenate(xin, axis=1), name='embedding_concat')(emb)\n",
    "# # print(lamb.shape)\n",
    "# emb = K.squeeze(emb, axis=2)\n",
    "# print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "#     3D tensor with shape: `(batch, steps, channels)`\n",
    "\n",
    "# Output shape\n",
    "#     3D tensor with shape: `(batch, new_steps, filters)`\n",
    "#     `steps` value might have changed due to padding or strides.\n",
    "\n",
    "# Specify each convolution layer and their kernel size i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "maxp1_1 = MaxPooling1D(pool_size=pool_size)(btch1_1)\n",
    "flat1_1 = Flatten()(maxp1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "maxp1_2 = MaxPooling1D(pool_size=pool_size)(btch1_2)\n",
    "flat1_2 = Flatten()(maxp1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "maxp1_3 = MaxPooling1D(pool_size=pool_size)(btch1_3)\n",
    "flat1_3 = Flatten()(maxp1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([flat1_1, flat1_2, flat1_3], axis=1)\n",
    "drp1 = Dropout(0)(cnct)\n",
    "\n",
    "dns1  = Dense(128, activation='relu')(drp1)\n",
    "out = Dense(num_classes, activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 5, 300)       900000000   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3, 10)        9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3, 10)        9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3, 10)        9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 3, 10)        40          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3, 10)        40          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 3, 10)        40          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 10)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 10)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 10)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          3968        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            645         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 900,031,763\n",
      "Trainable params: 31,703\n",
      "Non-trainable params: 900,000,060\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 420143 samples, validate on 46683 samples\n",
      "Epoch 1/10\n",
      "420143/420143 [==============================] - 10s 24us/step - loss: 0.5582 - acc: 0.8043 - val_loss: 0.5035 - val_acc: 0.8239\n",
      "Epoch 2/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4705 - acc: 0.8356 - val_loss: 0.4807 - val_acc: 0.8337\n",
      "Epoch 3/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4463 - acc: 0.8440 - val_loss: 0.4708 - val_acc: 0.8371\n",
      "Epoch 4/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4319 - acc: 0.8491 - val_loss: 0.4696 - val_acc: 0.8401\n",
      "Epoch 5/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4216 - acc: 0.8531 - val_loss: 0.4691 - val_acc: 0.8375\n",
      "Epoch 6/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4140 - acc: 0.8556 - val_loss: 0.4652 - val_acc: 0.8407\n",
      "Epoch 7/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4078 - acc: 0.8577 - val_loss: 0.4654 - val_acc: 0.8400\n",
      "Epoch 8/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.4028 - acc: 0.8597 - val_loss: 0.4646 - val_acc: 0.8407\n",
      "Epoch 9/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.3986 - acc: 0.8609 - val_loss: 0.4647 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "420143/420143 [==============================] - 9s 21us/step - loss: 0.3949 - acc: 0.8626 - val_loss: 0.4663 - val_acc: 0.8429\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
