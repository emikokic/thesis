{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se explorarán, en pequeños experimentos, distintas formas de representación de los datos del corpus WiNER (Ghaddar y Langlais 2017) para utilizarlos en la tarea de reconocimiento de entidades nombradas. Para esto se exploran distintas combinaciones de vectores de palabras como representación de una instancia de entrenamiento (Iacobacci et al. 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del Corpus WiNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Documents.tar.bz2 : este archivo contiene 3239540 artículos de Wikipedia repartidos en 3223 archivos. Cantidad de oraciones: 54607542. Cada archivo contiene aproximadamente 1000 artículos nombrados por sus respectivos IDs. Los artículos están indexados por su wikiID seguidos de oraciones (una por línea), donde las palabras son remplazadas por sus ids.\n",
    "\n",
    "      ID <number>\n",
    "      1234 4522 23 4 4567\n",
    "      456 21 9890 123 7 0\n",
    "\n",
    "* document.vocab : este archivo contiene el mapeo de palabras (case sensitive); el formato es: \n",
    " \n",
    "      palabra #ocurrencias\n",
    "    \n",
    "  El ID de cada palabra es el número de línea en la cual ocurre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from corpus import spread_artID\n",
    "from embeddings import concat_vectors, mean_vectors, fractional_decay, exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = pd.read_csv('./corpus_WiNER/document.vocab', sep=' ', header=None, \n",
    "                           names=['word', 'frequency'], keep_default_na=False) \n",
    "                        # con esto evito que el parser trate como nan value a algunas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_mapping.shape)\n",
    "word_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_mapping['word'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los artículos del Documento \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = pd.read_csv('./corpus_WiNER/Documents/0', sep='ID', engine='python', header=None, names=['sentence', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a asociarle a cada oración el ID del artículo al cual pertenece.\n",
    "\n",
    "Es importante recordar que el orden de las oraciones está dado por los índices del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_ID_list = doc_0['art_ID'].tolist()\n",
    "art_ID = 0\n",
    "for idx, elem in enumerate(art_ID_list):\n",
    "    if not np.isnan(elem):\n",
    "        art_ID = elem\n",
    "    else:\n",
    "        art_ID_list[idx] = art_ID\n",
    "doc_0['art_ID'] = art_ID_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos ahora las filas con NaN que contenian los ID de los artículos inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = doc_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El documento contiene {} oraciones.'.format(doc_0.shape[0]))\n",
    "print('El documento contiene {} artículos'.format(len(doc_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos que cada sentencia sea una lista de palabras codificadas y casteamos a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0['sentence'] = doc_0['sentence'].map(lambda x: list(map(int, x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con un artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruimos la primera oración del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_decoder(sentence, word_mapping):\n",
    "    dec_sentence = []\n",
    "    for idx in sentence:\n",
    "        mapped_w = word_mapping.loc[idx, 'word']\n",
    "        dec_sentence.append(mapped_w)\n",
    "    return dec_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos article.columns.get_loc('sentence') para evitar hardcodear el índice correspondiente\n",
    "# a la columna 'sentence' que en este caso es 0\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings utilizando el modelo pre-entrenado word2vec de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la librería Gensim https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos Google's pre-trained Word2Vec model.\n",
    "\n",
    "Utilizando KeyedVectors para cargar el modelo tiene la desventaja de que no se puede seguir entrenando. Pero es más eficiente que utilizar gensim.models.Word2Vec\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# model = KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save('./models/word2vecGoogle.model')\n",
    "w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')\n",
    "end = time.time()\n",
    "print('demora: {}'.format(end-start))\n",
    "# model = Word2Vec.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de word embeddings: {}'.format(len(w2v_model.vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensionalidad de los vectores: {}'.format(w2v_model.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos distintas combinaciones de vectores de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación\n",
    "\n",
    "Este método consiste en concatenar los vectores de palabras que rodean una palabra objetivo en un vector más grande, que tiene un tamaño igual a las dimensiones agregadas de todos las proyecciones (embeddings) individuales.\n",
    "\n",
    "- $w_{ij}$ = peso asociado con la i-ésima dimensión del vector de la j-ésima palabra en la oración. NOTA: con los vectores de palabras de una oración se forma una matriz $w^{\\space D\\space x\\space L}$ donde $L$ es la cantidad de palabras de esa oración.\n",
    "- $D$ = dimensionalidad de los word vectors originales. Por ejemplo, al usar el modelo word2vec de Google se tiene $D$ = 300.\n",
    "- $W$ = tamaño de ventana que se define como el número de palabras en un solo lado.\n",
    "\n",
    "Nos interesa representar el contexto de la I-ésima palabra de la oración. \n",
    "\n",
    "La i-ésima dimensión del vector de concatenación, que tiene un tamaño de $2 W D$, se calcula de la siguiente manera:\n",
    "\n",
    "$$ e_{i} =\\begin{cases} \n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor < W \\\\\n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space 1\\space  +\\space\\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & c.c.\n",
    "   \\end{cases}$$\n",
    "   \n",
    "\n",
    "<br>\n",
    "Al tomar una ventana simétrica, se realiza un relleno (padding) con ceros a izquierda y/o derecha según corresponda para mantener la misma dimesionalidad en cada nuevo vector generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promedio\n",
    "\n",
    "Como su nombre indica, se calcula el centroide de los embeddings de todas las palabras circundantes. La fórmula divide cada dimensión en $2W$ ya que el número de palabras del contexto es dos veces el tamaño de la ventana:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} \\frac{w_{ij}}{2W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento fraccional\n",
    "\n",
    "Una tercera estrategia para construir un vector de carácteristicas en base a los embeddings de palabras contextuales está inspirada en la forma en que Word2vec combina las palabras en el contexto. Aquí, se supone que la importancia de una palabra para nuestra representación es inversamente proporcional a su distancia respecto a la palabra objetivo.\n",
    "\n",
    "Por lo tanto, las palabras contextuales se ponderan en función de su distancia de la palabra objetivo:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} *\\frac{W - \\lvert I-j\\rvert}{W}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento exponencial\n",
    "\n",
    "Funciona de manera similar al decaimiento fraccional, que le da más importancia al contexto cercano, pero en este caso la ponderación se realiza exponencialmente:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} * (1 - \\alpha)^{\\lvert \\space I\\space-\\space j\\space\\rvert\\space-\\space1}$$\n",
    "\n",
    "donde $\\alpha = 1 - 0.1^{(W-1)^{-1}}$ es el parámetro de decaimiento. Elegimos el parámetro de tal manera que las palabras inmediatas que rodean a la palabra objetivo contribuyen 10 veces más que las últimas palabras en ambos lados de la ventana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos CoarseNE.tar.bz2\n",
    "\n",
    "Contiene menciones anotadas automáticamente con etiquetas de entidades nombradas (PER, LOC, ORG y MISC).\n",
    "\n",
    "El formato es:\n",
    "\n",
    "    ID artID\n",
    "    sentIdx begin end entityType\n",
    "    \n",
    "donde entityType[0] = PER | entityType[1] = LOC | entityType[2] = ORG | entityType[3] = MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = pd.read_csv('./corpus_WiNER/CoarseNE/0', sep='ID', engine='python', header=None, names=['named-entity', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coarseNE_0.shape)\n",
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el mismo truco que utilizamos en los documentos para propagar los art_ID según corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = spread_artID(coarseNE_0)\n",
    "coarseNE_0 = coarseNE_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coarseNE_0 contiene {} entidades.'.format(coarseNE_0.shape[0]))\n",
    "print('coarseNE_0 contiene {} artículos'.format(len(coarseNE_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuevas columnas con la info de la columna named-entity para mejor manipulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(x):\n",
    "    tags = ['PER', 'LOC', 'ORG', 'MISC']\n",
    "    return tags[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0['named-entity'] = coarseNE_0['named-entity'].map(lambda x: x.split('\\t'))\n",
    "coarseNE_0['senIdx'] = coarseNE_0['named-entity'].map(lambda x: int(x[0]))\n",
    "coarseNE_0['begin'] = coarseNE_0['named-entity'].map(lambda x: int(x[1]))\n",
    "coarseNE_0['end'] = coarseNE_0['named-entity'].map(lambda x: int(x[2]))\n",
    "coarseNE_0['entityType'] = coarseNE_0['named-entity'].map(lambda x: get_tag(int(x[3])))\n",
    "coarseNE_0 = coarseNE_0.drop(columns='named-entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los artículos que ocurren en Documents/0 se encuentran en CoarseNE/0\n",
    "\n",
    "Notar que esto sucede porque esos artículos no contienen entidades nombradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Artículos que están presentes en Documents/0 pero no en CoarseNE/0: {}'\n",
    "      .format(list(set(doc_0.art_ID.unique()) - set(coarseNE_0.art_ID.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 431177]\n",
    "for sentence in article.sentence.values:\n",
    "    dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "    print(' '.join(dec_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos como están anotadas las entidades nombradas de una oración en particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]\n",
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_entities = coarseNE_0[coarseNE_0.art_ID == 1000]\n",
    "entities_sen_0 = art_entities[art_entities.senIdx == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in entities_sen_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora utilicemos los dataframes preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc77_df = pd.read_pickle('./corpus_WiNER/docs_df/doc_77')\n",
    "article_df = doc77_df[doc77_df.art_ID == 145492]\n",
    "dec_sentence = article_df.iloc[0, article_df.columns.get_loc('sentence')]\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_77 = pd.read_pickle('./corpus_WiNER/coarseNE_df/coarseNE_77')\n",
    "art_entities_df = coarseNE_77[coarseNE_77.art_ID == 145492]\n",
    "sen_entities_0 = art_entities_df[art_entities_df.senIdx == 0]\n",
    "art_entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityListFromSentence(senIdx, sen_length, art_entities_df):\n",
    "    # We take the df with the entities of each sentence\n",
    "    sen_entities_df = art_entities_df[art_entities_df.senIdx == senIdx]\n",
    "    # An empty dataframe means that the sentence doesn't have any entity\n",
    "    if sen_entities_df.empty:\n",
    "        entities = ['O' for _ in range(sen_length)]\n",
    "    else:\n",
    "        entities = []\n",
    "        i = 0\n",
    "        for _, row in sen_entities_df.iterrows():\n",
    "            while i < row['begin']:\n",
    "                entities.append('O')\n",
    "                i += 1\n",
    "            while i < row['end']:\n",
    "                entities.append(row['entityType'])\n",
    "                i += 1\n",
    "        while i < sen_length:\n",
    "            entities.append('O')\n",
    "            i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in sen_entities_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = entityListFromSentence(senIdx=0, sen_length=len(dec_sentence),\n",
    "                                     art_entities_df=art_entities_df)\n",
    "print(' '.join(article_df['sentence'][1]))\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_strategy(strategy, sentence, W, w2v_model):\n",
    "    return {\n",
    "        'concat': concat_vectors(sentence, W, w2v_model), \n",
    "        'mean': mean_vectors(sentence, W, w2v_model), \n",
    "        'frac_decay': fractional_decay(sentence, W, w2v_model),\n",
    "        'exp_decay': exponential_decay(sentence, W, w2v_model),\n",
    "    }[strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(proc_chunk, f_name, art_entities_df):\n",
    "    if f_name == 'w2v':\n",
    "        fun = lambda sentence: w2v_strategy('exp_decay', sentence, 5, w2v_model)\n",
    "        chunk_res = proc_chunk['sentence'].map(fun)\n",
    "    elif f_name == 'get_entity':\n",
    "        fun = lambda senIdx: entityListFromSentence(senIdx, proc_chunk.loc[senIdx, 'sen_length'],\n",
    "                                                    art_entities_df)\n",
    "        chunk_res = proc_chunk.index.map(fun)\n",
    "    else:\n",
    "        raise Exception\n",
    "    chunk_res.index = proc_chunk.index\n",
    "    return chunk_res\n",
    "\n",
    "def get_proc_chunks(df, n_proc):\n",
    "    chunksize = len(df) // n_proc\n",
    "    proc_chunks = []\n",
    "    for i_proc in range(n_proc):\n",
    "        chunkstart = i_proc * chunksize\n",
    "        # make sure to include the division remainder for the last process\n",
    "        chunkend = (i_proc + 1) * chunksize if i_proc < n_proc - 1 else None\n",
    "        proc_chunks.append(df.iloc[slice(chunkstart, chunkend)])\n",
    "    return proc_chunks\n",
    "\n",
    "def run(proc_chunks, f_name, art_entities_df):\n",
    "    with Pool(processes=n_proc) as pool:\n",
    "        # starts the sub-processes without blocking\n",
    "        # pass the chunk to each worker process\n",
    "        proc_results = [pool.apply_async(process_chunk,\n",
    "                                         args=(chunk,f_name,art_entities_df))\n",
    "                        for chunk in proc_chunks]\n",
    "        # blocks until all results are fetched\n",
    "        result_chunks = [r.get() for r in proc_results]\n",
    "    return result_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = article_df\n",
    "n_proc = 5 #cpu_count() #Number of CPU cores on your system\n",
    "fun = lambda sentence: w2v_strategy('exp_decay', sentence, 5, w2v_model)\n",
    "proc_chunks = get_proc_chunks(df, n_proc)\n",
    "result_chunks = run(proc_chunks, 'w2v', art_entities_df)\n",
    "art_vectors = pd.concat(result_chunks)\n",
    "art_vectors = list(itertools.chain(*art_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(art_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector_EntityFromArticle(article_df, art_entities_df, strategy, W, w2v_model):\n",
    "    '''@return: filled DataFrame with columns {wordVector, entityType}'''\n",
    "    article_df = article_df.reset_index(drop=True)\n",
    "    article_df['sen_length'] = article_df['sentence'].map(lambda x: len(x))\n",
    "\n",
    "    n_proc = 5 #cpu_count() #Number of CPU cores on your system\n",
    "\n",
    "    proc_chunks = get_proc_chunks(article_df, n_proc)\n",
    "    result_chunks = run(proc_chunks, 'w2v', art_entities_df)\n",
    "    art_vectors = pd.concat(result_chunks)\n",
    "\n",
    "    result_chunks = run(proc_chunks, 'get_entity', art_entities_df)\n",
    "    art_entities = result_chunks\n",
    "    # Fastest way to flatten list of arrays\n",
    "    art_vectors = list(itertools.chain(*art_vectors))\n",
    "    art_entities = list(itertools.chain(*art_entities))\n",
    "    art_entities = list(itertools.chain(*art_entities))\n",
    "    wordVector_Entity_df = pd.DataFrame({'wordVector': art_vectors, \n",
    "                                         'entityType': art_entities})\n",
    "\n",
    "    return wordVector_Entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                   strategy='exp_decay',\n",
    "                                                   W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('demora:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = list(wordVector_Entity_df['entityType'])\n",
    "tokens = len(entity_list)\n",
    "print('Cantidad de palabras: {}'.format(tokens))\n",
    "n_e = len(entity_list) - entity_list.count('O')\n",
    "print('Cantidad de entidades nombradas: {}'.format(n_e))\n",
    "print('Porcentaje de entidades nombradas: {:.2f}%'.format(n_e/tokens*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, cada artículo contiene una proporción reducida de palabras que son entidades.\n",
    "\n",
    "Una alternativa para evitar cómputo y uso de memoria podría ser eliminar una proporción de vectores que no representan ninguna entidad (etiquetados con 'O') a la hora de construir la matriz de palabra - entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_entities(df, frac):\n",
    "    '''\n",
    "    Remove a fraction of non entities vectors (entityType == 'O')\n",
    "    df: wordVector_Entity_df\n",
    "    frac: float value between 0 and 1\n",
    "    @return df with a fraction of the non entities rows removed\n",
    "    '''\n",
    "    sample = df[df.entityType == 'O'].sample(frac=frac, random_state=77)\n",
    "    df = df.drop(index=sample.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVector_Entity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_non_entities(wordVector_Entity_df.head(10), 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWordVectors_Entity(doc_filename, coarseNE_filename, strategy, W, w2v_model):\n",
    "    '''\n",
    "    Creates a N x D matrix of word vectors and saves it to disk. \n",
    "    Creates an 1 x N matrix of entities and saves it to disk. \n",
    "    N is the number of words in the document.\n",
    "    D is the size of each word vector.\n",
    "    The entity types are: PER - LOC - ORG - MISC - O\n",
    "    \n",
    "    strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "    W: window size\n",
    "    w2v_model: pre-trained word2vec model\n",
    "    '''\n",
    "    wordVectors = []\n",
    "    entityVector = []\n",
    "    count = 0 # Used in the 'progress bar'    \n",
    "    doc_df = pd.read_pickle('./corpus_WiNER/docs_df/'+ doc_filename)\n",
    "    coarseNE_df = pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ coarseNE_filename)\n",
    "    art_IDs = coarseNE_df.art_ID.unique()\n",
    "    \n",
    "    print('starting')\n",
    "    start = time.time()\n",
    "    print('doc_df shape:', doc_df.shape)\n",
    "    n_proc = 20 #cpu_count() #Number of CPU cores on your system\n",
    "    proc_chunks = get_proc_chunks(doc_df, n_proc)\n",
    "    print('len proc_chunks:', len(proc_chunks[0]))\n",
    "    result_chunks = run(proc_chunks, 'w2v', coarseNE_df)\n",
    "    print('finishing')\n",
    "    end = time.time()\n",
    "    print('demora:', end-start)\n",
    "    art_vectors = pd.concat(result_chunks)\n",
    "    print(type(art_vectors))\n",
    "    print('shapeee:', art_vectors.shape)\n",
    "    \n",
    "    \n",
    "    # We consider only the articles with at least one entity.\n",
    "    # That's why we iterate over the coarseNE's articles.\n",
    "    for art_ID in np.nditer(art_IDs):\n",
    "        article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "        art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]     \n",
    "        wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                           strategy, W, w2v_model)         \n",
    "        wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.90)\n",
    "        wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "        entityVector += list(wordVector_Entity_df['entityType'])\n",
    "\n",
    "        if count % 10 == 0: # Kind of progress bar :P\n",
    "            print(count, end=' ')\n",
    "        count += 1\n",
    "\n",
    "    starting = time.time()\n",
    "    np.savez_compressed('./corpus_WiNER/entity_vectors/ev_' + doc_filename + '_' + strategy,\n",
    "                        entityVector)\n",
    "    np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+ doc_filename + '_' + strategy,\n",
    "                        *wordVectors)\n",
    "    finishing = time.time()\n",
    "    print('tiempo de guardado:', finishing - starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "genWordVectors_Entity(doc_filename='doc_0', coarseNE_filename='coarseNE_0',\n",
    "                      strategy='exp_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = np.load('./corpus_WiNER/word_vectors/wv_doc_0_exp_decay_.npz')\n",
    "word_vectors = np.load('./corpus_WiNER/wv_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, arr in word_vectors.iteritems():\n",
    "    print(a_name, len(arr))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_doc_0_exp_decay_.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/ev_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, arr in entity_vector.iteritems():\n",
    "    print(a_name, len(arr))\n",
    "    entities = list(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.count('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createWordVectors_Entity(strategy, W):\n",
    "#     '''  TODO: change description!\n",
    "#     Create a N x (D+1) matrix where N is the total amount of words and \n",
    "#     D is the dimensionality of each word vector. \n",
    "#     The values of the last column are the entity type (PER - LOC - ORG - MISC - O).\n",
    "    \n",
    "#     strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "#     W: window size\n",
    "    \n",
    "#     @return wordVectors_Entity matrix\n",
    "#     '''\n",
    "#     w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')\n",
    "#     D = w2v_model.vector_size\n",
    "#     count = 0 # Used in the 'progress bar'\n",
    "#     doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "#     coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "#     doc_filenames.sort()\n",
    "#     coarseNE_filenames.sort()\n",
    "#     #TODO: remove the following after optimization\n",
    "#     starting = time.time()\n",
    "    \n",
    "    \n",
    "#     # TODO: modularizar: que el for interno sea otro metodo\n",
    "    \n",
    "#     for doc_f, coarseNE_f in zip(doc_filenames, coarseNE_filenames):\n",
    "#         wordVectors = []\n",
    "#         entityVector = []\n",
    "    \n",
    "#         start = time.time()\n",
    "        \n",
    "#         doc_df = pd.read_pickle('./corpus_WiNER/docs_df/'+ doc_f)\n",
    "#         coarseNE_df = pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ coarseNE_f)\n",
    "#         art_IDs = coarseNE_df.art_ID.unique()\n",
    "\n",
    "#         end = time.time()\n",
    "#         print('time 0: ', end-start)        \n",
    "        \n",
    "#         # We consider only the articles with at least one entity.\n",
    "#         # That's why we iterate over the coarseNE's articles.\n",
    "#         for art_ID in np.nditer(art_IDs):\n",
    "                        \n",
    "#             article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "#             art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]\n",
    "\n",
    "#             start = time.time()            \n",
    "#             wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "#                                                                strategy, D, W, w2v_model)\n",
    "#             end = time.time()\n",
    "#             print('wordVector_Entity_df: ', end-start) \n",
    "            \n",
    "            \n",
    "# #             start = time.time()            \n",
    "#             wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.90)\n",
    "# #             end = time.time()\n",
    "# #             print('dropping: ', end-start)                     \n",
    "            \n",
    "#             wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "#             entityVector += list(wordVector_Entity_df['entityType'])\n",
    "            \n",
    "# #             if count % 10 == 0:\n",
    "# #                 print(count, end=' ')\n",
    "# #             count += 1\n",
    "\n",
    "        \n",
    "#         finishing = time.time()\n",
    "#         print('tiempo de procesamiento:', finishing - starting)\n",
    "\n",
    "#         starting = time.time()\n",
    "#         np.savez_compressed('./corpus_WiNER/entity_vectors/ev_'+doc_f+'_'+strategy+'_', entityVector)\n",
    "#         np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+doc_f+'_'+strategy+'_', *wordVectors)\n",
    "\n",
    "#         finishing = time.time()\n",
    "#         print('tiempo de guardado:', finishing - starting)\n",
    "#         # Probamos sin el * => guardar en un array de arrays \n",
    "# #         np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+doc_f, wordVectors)\n",
    "      \n",
    "#         break\n",
    "#     print('End process')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
