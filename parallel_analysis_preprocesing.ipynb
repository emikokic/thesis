{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se explorarán, en pequeños experimentos, distintas formas de representación de los datos del corpus WiNER (Ghaddar y Langlais 2017) para utilizarlos en la tarea de reconocimiento de entidades nombradas. Para esto se exploran distintas combinaciones de vectores de palabras como representación de una instancia de entrenamiento (Iacobacci et al. 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del Corpus WiNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Documents.tar.bz2 : este archivo contiene 3239540 artículos de Wikipedia repartidos en 3223 archivos. Cantidad de oraciones: 54607542. Cada archivo contiene aproximadamente 1000 artículos nombrados por sus respectivos IDs. Los artículos están indexados por su wikiID seguidos de oraciones (una por línea), donde las palabras son remplazadas por sus ids.\n",
    "\n",
    "      ID <number>\n",
    "      1234 4522 23 4 4567\n",
    "      456 21 9890 123 7 0\n",
    "\n",
    "* document.vocab : este archivo contiene el mapeo de palabras (case sensitive); el formato es: \n",
    " \n",
    "      palabra #ocurrencias\n",
    "    \n",
    "  El ID de cada palabra es el número de línea en la cual ocurre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from corpus import spread_artID\n",
    "from embeddings import concat_vectors, mean_vectors, fractional_decay, exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "word_mapping = pd.read_csv('./corpus_WiNER/document.vocab', sep=' ', header=None, \n",
    "                           names=['word', 'frequency'], keep_default_na=False) \n",
    "                        # con esto evito que el parser trate como nan value a algunas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2021984, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>73411953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>68044881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>54241850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>40550466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>34602894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word frequency\n",
       "0  the  73411953\n",
       "1    ,  68044881\n",
       "2    .  54241850\n",
       "3   of  40550466\n",
       "4  and  34602894"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_mapping.shape)\n",
    "word_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021983"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_mapping['word'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los artículos del Documento \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = pd.read_csv('./corpus_WiNER/Documents/0', sep='ID', engine='python', header=None, names=['sentence', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130209, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650 5590 753942 12 189 243 1 2039 47 257 682 1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34 23207 1523 4 57193 15 10777 14353 4 157019 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>418 0 1858 101 0 1322 1 5590 18 860 729 14 92 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>650 5590 1523 8 1136 15 11301 575 1156 1 2746 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    art_ID\n",
       "0                                                NaN  431388.0\n",
       "1  650 5590 753942 12 189 243 1 2039 47 257 682 1...       NaN\n",
       "2  34 23207 1523 4 57193 15 10777 14353 4 157019 ...       NaN\n",
       "3  418 0 1858 101 0 1322 1 5590 18 860 729 14 92 ...       NaN\n",
       "4  650 5590 1523 8 1136 15 11301 575 1156 1 2746 ...       NaN"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a asociarle a cada oración el ID del artículo al cual pertenece.\n",
    "\n",
    "Es importante recordar que el orden de las oraciones está dado por los índices del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_ID_list = doc_0['art_ID'].tolist()\n",
    "art_ID = 0\n",
    "for idx, elem in enumerate(art_ID_list):\n",
    "    if not np.isnan(elem):\n",
    "        art_ID = elem\n",
    "    else:\n",
    "        art_ID_list[idx] = art_ID\n",
    "doc_0['art_ID'] = art_ID_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos ahora las filas con NaN que contenian los ID de los artículos inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = doc_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento contiene 128395 oraciones.\n",
      "El documento contiene 1814 artículos\n"
     ]
    }
   ],
   "source": [
    "print('El documento contiene {} oraciones.'.format(doc_0.shape[0]))\n",
    "print('El documento contiene {} artículos'.format(len(doc_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos que cada sentencia sea una lista de palabras codificadas y casteamos a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0['sentence'] = doc_0['sentence'].map(lambda x: list(map(int, x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[650, 5590, 753942, 12, 189, 243, 1, 2039, 47,...</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[34, 23207, 1523, 4, 57193, 15, 10777, 14353, ...</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[418, 0, 1858, 101, 0, 1322, 1, 5590, 18, 860,...</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[650, 5590, 1523, 8, 1136, 15, 11301, 575, 115...</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[152, 642, 16, 8143, 23122, 20, 0, 662955, 16,...</td>\n",
       "      <td>431388.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    art_ID\n",
       "1  [650, 5590, 753942, 12, 189, 243, 1, 2039, 47,...  431388.0\n",
       "2  [34, 23207, 1523, 4, 57193, 15, 10777, 14353, ...  431388.0\n",
       "3  [418, 0, 1858, 101, 0, 1322, 1, 5590, 18, 860,...  431388.0\n",
       "4  [650, 5590, 1523, 8, 1136, 15, 11301, 575, 115...  431388.0\n",
       "5  [152, 642, 16, 8143, 23122, 20, 0, 662955, 16,...  431388.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con un artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>[63903, 24730, 9, 7, 3035, 4104, 7584, 1, 355,...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>[3654, 16, 2502, 39413, 1, 24730, 9, 44, 3, 99...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>[24730, 35, 49, 3521, 15, 575, 1, 15, 2440, 1,...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>[152, 112, 8, 2037, 20, 52, 54, 3035, 17572, 3...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>[64, 62, 5787, 1132, 15, 0, 144, 24730, 1312, ...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  art_ID\n",
       "2756  [63903, 24730, 9, 7, 3035, 4104, 7584, 1, 355,...  1000.0\n",
       "2757  [3654, 16, 2502, 39413, 1, 24730, 9, 44, 3, 99...  1000.0\n",
       "2758  [24730, 35, 49, 3521, 15, 575, 1, 15, 2440, 1,...  1000.0\n",
       "2759  [152, 112, 8, 2037, 20, 52, 54, 3035, 17572, 3...  1000.0\n",
       "2760  [64, 62, 5787, 1132, 15, 0, 144, 24730, 1312, ...  1000.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruimos la primera oración del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_decoder(sentence, word_mapping):\n",
    "    dec_sentence = []\n",
    "    for idx in sentence:\n",
    "        mapped_w = word_mapping.loc[idx, 'word']\n",
    "        dec_sentence.append(mapped_w)\n",
    "    return dec_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos article.columns.get_loc('sentence') para evitar hardcodear el índice correspondiente\n",
    "# a la columna 'sentence' que en este caso es 0\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hercule Poirot is a fictional Belgian detective , created by Agatha Christie .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings utilizando el modelo pre-entrenado word2vec de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la librería Gensim https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos Google's pre-trained Word2Vec model.\n",
    "\n",
    "Utilizando KeyedVectors para cargar el modelo tiene la desventaja de que no se puede seguir entrenando. Pero es más eficiente que utilizar gensim.models.Word2Vec\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demora: 9.04468035697937\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# model = KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save('./models/word2vecGoogle.model')\n",
    "w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')\n",
    "end = time.time()\n",
    "print('demora: {}'.format(end-start))\n",
    "# model = Word2Vec.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de word embeddings: 3000000\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de word embeddings: {}'.format(len(w2v_model.vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de los vectores: 300\n"
     ]
    }
   ],
   "source": [
    "print('Dimensionalidad de los vectores: {}'.format(w2v_model.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos distintas combinaciones de vectores de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación\n",
    "\n",
    "Este método consiste en concatenar los vectores de palabras que rodean una palabra objetivo en un vector más grande, que tiene un tamaño igual a las dimensiones agregadas de todos las proyecciones (embeddings) individuales.\n",
    "\n",
    "- $w_{ij}$ = peso asociado con la i-ésima dimensión del vector de la j-ésima palabra en la oración. NOTA: con los vectores de palabras de una oración se forma una matriz $w^{\\space D\\space x\\space L}$ donde $L$ es la cantidad de palabras de esa oración.\n",
    "- $D$ = dimensionalidad de los word vectors originales. Por ejemplo, al usar el modelo word2vec de Google se tiene $D$ = 300.\n",
    "- $W$ = tamaño de ventana que se define como el número de palabras en un solo lado.\n",
    "\n",
    "Nos interesa representar el contexto de la I-ésima palabra de la oración. \n",
    "\n",
    "La i-ésima dimensión del vector de concatenación, que tiene un tamaño de $2 W D$, se calcula de la siguiente manera:\n",
    "\n",
    "$$ e_{i} =\\begin{cases} \n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor < W \\\\\n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space 1\\space  +\\space\\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & c.c.\n",
    "   \\end{cases}$$\n",
    "   \n",
    "\n",
    "<br>\n",
    "Al tomar una ventana simétrica, se realiza un relleno (padding) con ceros a izquierda y/o derecha según corresponda para mantener la misma dimesionalidad en cada nuevo vector generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promedio\n",
    "\n",
    "Como su nombre indica, se calcula el centroide de los embeddings de todas las palabras circundantes. La fórmula divide cada dimensión en $2W$ ya que el número de palabras del contexto es dos veces el tamaño de la ventana:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} \\frac{w_{ij}}{2W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento fraccional\n",
    "\n",
    "Una tercera estrategia para construir un vector de carácteristicas en base a los embeddings de palabras contextuales está inspirada en la forma en que Word2vec combina las palabras en el contexto. Aquí, se supone que la importancia de una palabra para nuestra representación es inversamente proporcional a su distancia respecto a la palabra objetivo.\n",
    "\n",
    "Por lo tanto, las palabras contextuales se ponderan en función de su distancia de la palabra objetivo:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} *\\frac{W - \\lvert I-j\\rvert}{W}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento exponencial\n",
    "\n",
    "Funciona de manera similar al decaimiento fraccional, que le da más importancia al contexto cercano, pero en este caso la ponderación se realiza exponencialmente:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} * (1 - \\alpha)^{\\lvert \\space I\\space-\\space j\\space\\rvert\\space-\\space1}$$\n",
    "\n",
    "donde $\\alpha = 1 - 0.1^{(W-1)^{-1}}$ es el parámetro de decaimiento. Elegimos el parámetro de tal manera que las palabras inmediatas que rodean a la palabra objetivo contribuyen 10 veces más que las últimas palabras en ambos lados de la ventana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos CoarseNE.tar.bz2\n",
    "\n",
    "Contiene menciones anotadas automáticamente con etiquetas de entidades nombradas (PER, LOC, ORG y MISC).\n",
    "\n",
    "El formato es:\n",
    "\n",
    "    ID artID\n",
    "    sentIdx begin end entityType\n",
    "    \n",
    "donde entityType[0] = PER | entityType[1] = LOC | entityType[2] = ORG | entityType[3] = MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = pd.read_csv('./corpus_WiNER/CoarseNE/0', sep='ID', engine='python', header=None, names=['named-entity', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259470, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>named-entity</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0\\t0\\t2\\t0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0\\t5\\t6\\t1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\t10\\t12\\t0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1\\t2\\t4\\t0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   named-entity  art_ID\n",
       "0           NaN  1000.0\n",
       "1    0\\t0\\t2\\t0     NaN\n",
       "2    0\\t5\\t6\\t1     NaN\n",
       "3  0\\t10\\t12\\t0     NaN\n",
       "4    1\\t2\\t4\\t0     NaN"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(coarseNE_0.shape)\n",
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el mismo truco que utilizamos en los documentos para propagar los art_ID según corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = spread_artID(coarseNE_0)\n",
    "coarseNE_0 = coarseNE_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarseNE_0 contiene 257656 entidades.\n",
      "coarseNE_0 contiene 1810 artículos\n"
     ]
    }
   ],
   "source": [
    "print('coarseNE_0 contiene {} entidades.'.format(coarseNE_0.shape[0]))\n",
    "print('coarseNE_0 contiene {} artículos'.format(len(coarseNE_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuevas columnas con la info de la columna named-entity para mejor manipulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(x):\n",
    "    tags = ['PER', 'LOC', 'ORG', 'MISC']\n",
    "    return tags[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0['named-entity'] = coarseNE_0['named-entity'].map(lambda x: x.split('\\t'))\n",
    "coarseNE_0['senIdx'] = coarseNE_0['named-entity'].map(lambda x: int(x[0]))\n",
    "coarseNE_0['begin'] = coarseNE_0['named-entity'].map(lambda x: int(x[1]))\n",
    "coarseNE_0['end'] = coarseNE_0['named-entity'].map(lambda x: int(x[2]))\n",
    "coarseNE_0['entityType'] = coarseNE_0['named-entity'].map(lambda x: get_tag(int(x[3])))\n",
    "coarseNE_0 = coarseNE_0.drop(columns='named-entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_ID</th>\n",
       "      <th>senIdx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   art_ID  senIdx  begin  end entityType\n",
       "1  1000.0       0      0    2        PER\n",
       "2  1000.0       0      5    6        LOC\n",
       "3  1000.0       0     10   12        PER\n",
       "4  1000.0       1      2    4        PER\n",
       "5  1000.0       1      5    6        PER"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los artículos que ocurren en Documents/0 se encuentran en CoarseNE/0\n",
    "\n",
    "Notar que esto sucede porque esos artículos no contienen entidades nombradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos que están presentes en Documents/0 pero no en CoarseNE/0: [431177.0, 432375.0, 432318.0, 10263.0]\n"
     ]
    }
   ],
   "source": [
    "print('Artículos que están presentes en Documents/0 pero no en CoarseNE/0: {}'\n",
    "      .format(list(set(doc_0.art_ID.unique()) - set(coarseNE_0.art_ID.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In economics , economics of location is a strategy used by firms in a monopolistic competition environment .\n",
      "Unlike a product differentiation strategy , where firms make their products different in order to attract customers , the economics of location strategy causes firms to produce similar or identical products .\n"
     ]
    }
   ],
   "source": [
    "article = doc_0[doc_0.art_ID == 431177]\n",
    "for sentence in article.sentence.values:\n",
    "    dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "    print(' '.join(dec_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos como están anotadas las entidades nombradas de una oración en particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hercule Poirot is a fictional Belgian detective , created by Agatha Christie .'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]\n",
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_entities = coarseNE_0[coarseNE_0.art_ID == 1000]\n",
    "entities_sen_0 = art_entities[art_entities.senIdx == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hercule Poirot : PER\n",
      "Belgian : LOC\n",
      "Agatha Christie : PER\n"
     ]
    }
   ],
   "source": [
    "for idx, row in entities_sen_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora utilicemos los dataframes preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Belton, House, is, a, Grade, I, listed, count...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, mansion, is, surrounded, by, formal, gar...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Belton, has, been, described, as, a, compilat...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, house, has, also, been, described, as, t...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Only, Brympton, d'Evercy, has, been, similarl...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    art_ID\n",
       "1  [Belton, House, is, a, Grade, I, listed, count...  145492.0\n",
       "2  [The, mansion, is, surrounded, by, formal, gar...  145492.0\n",
       "3  [Belton, has, been, described, as, a, compilat...  145492.0\n",
       "4  [The, house, has, also, been, described, as, t...  145492.0\n",
       "5  [Only, Brympton, d'Evercy, has, been, similarl...  145492.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc77_df = pd.read_pickle('./corpus_WiNER/docs_df/doc_77')\n",
    "article_df = doc77_df[doc77_df.art_ID == 145492]\n",
    "dec_sentence = article_df.iloc[0, article_df.columns.get_loc('sentence')]\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_ID</th>\n",
       "      <th>senIdx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42515</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42516</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42517</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42518</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42519</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         art_ID  senIdx  begin  end entityType\n",
       "42515  145492.0       0      0    2        LOC\n",
       "42516  145492.0       0     10   11        LOC\n",
       "42517  145492.0       0     12   13        LOC\n",
       "42518  145492.0       0     14   15        LOC\n",
       "42519  145492.0       0     16   17        LOC"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseNE_77 = pd.read_pickle('./corpus_WiNER/coarseNE_df/coarseNE_77')\n",
    "art_entities_df = coarseNE_77[coarseNE_77.art_ID == 145492]\n",
    "sen_entities_0 = art_entities_df[art_entities_df.senIdx == 0]\n",
    "art_entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityListFromSentence(senIdx, sen_length, art_entities_df):\n",
    "    # We take the df with the entities of each sentence\n",
    "    sen_entities_df = art_entities_df[art_entities_df.senIdx == senIdx]\n",
    "    # An empty dataframe means that the sentence doesn't have any entity\n",
    "    if sen_entities_df.empty:\n",
    "        entities = ['O' for _ in range(sen_length)]\n",
    "    else:\n",
    "        entities = []\n",
    "        i = 0\n",
    "        for _, row in sen_entities_df.iterrows():\n",
    "            while i < row['begin']:\n",
    "                entities.append('O')\n",
    "                i += 1\n",
    "            while i < row['end']:\n",
    "                entities.append(row['entityType'])\n",
    "                i += 1\n",
    "        while i < sen_length:\n",
    "            entities.append('O')\n",
    "            i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belton House : LOC\n",
      "Belton : LOC\n",
      "Grantham : LOC\n",
      "Lincolnshire : LOC\n",
      "England : LOC\n"
     ]
    }
   ],
   "source": [
    "for idx, row in sen_entities_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belton House is a Grade I listed country house in Belton near Grantham , Lincolnshire , England .\n",
      "['LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'LOC', 'O', 'LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "entity_list = entityListFromSentence(senIdx=0, sen_length=len(dec_sentence),\n",
    "                                     art_entities_df=art_entities_df)\n",
    "print(' '.join(article_df['sentence'][1]))\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_strategy(strategy, sentence, W, w2v_model):\n",
    "    return {\n",
    "        'concat': concat_vectors(sentence, W, w2v_model), \n",
    "        'mean': mean_vectors(sentence, W, w2v_model), \n",
    "        'frac_decay': fractional_decay(sentence, W, w2v_model),\n",
    "        'exp_decay': exponential_decay(sentence, W, w2v_model),\n",
    "    }[strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(proc_chunk, f_name, art_entities_df):\n",
    "    if f_name == 'w2v':\n",
    "        fun = lambda sentence: w2v_strategy('exp_decay', sentence, 5, w2v_model)\n",
    "        chunk_res = proc_chunk['sentence'].map(fun)\n",
    "    elif f_name == 'get_entity':\n",
    "        fun = lambda senIdx: entityListFromSentence(senIdx, proc_chunk.loc[senIdx, 'sen_length'],\n",
    "                                                    art_entities_df)\n",
    "        chunk_res = proc_chunk.index.map(fun)\n",
    "    else:\n",
    "        raise Exception\n",
    "    chunk_res.index = proc_chunk.index\n",
    "    return chunk_res\n",
    "\n",
    "def get_proc_chunks(df, n_proc):\n",
    "    chunksize = len(df) // n_proc\n",
    "    proc_chunks = []\n",
    "    for i_proc in range(n_proc):\n",
    "        chunkstart = i_proc * chunksize\n",
    "        # make sure to include the division remainder for the last process\n",
    "        chunkend = (i_proc + 1) * chunksize if i_proc < n_proc - 1 else None\n",
    "        proc_chunks.append(df.iloc[slice(chunkstart, chunkend)])\n",
    "    return proc_chunks\n",
    "\n",
    "def run(proc_chunks, f_name, art_entities_df):\n",
    "    with Pool(processes=n_proc) as pool:\n",
    "        # starts the sub-processes without blocking\n",
    "        # pass the chunk to each worker process\n",
    "        proc_results = [pool.apply_async(process_chunk,\n",
    "                                         args=(chunk,f_name,art_entities_df))\n",
    "                        for chunk in proc_chunks]\n",
    "        # blocks until all results are fetched\n",
    "        result_chunks = [r.get() for r in proc_results]\n",
    "    return result_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = article_df\n",
    "n_proc = 5 #cpu_count() #Number of CPU cores on your system\n",
    "fun = lambda sentence: w2v_strategy('exp_decay', sentence, 5, w2v_model)\n",
    "proc_chunks = get_proc_chunks(df, n_proc)\n",
    "result_chunks = run(proc_chunks, 'w2v', art_entities_df)\n",
    "art_vectors = pd.concat(result_chunks)\n",
    "art_vectors = list(itertools.chain(*art_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4395\n"
     ]
    }
   ],
   "source": [
    "print(len(art_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector_EntityFromArticle(article_df, art_entities_df, strategy, W, w2v_model):\n",
    "    '''@return: filled DataFrame with columns {wordVector, entityType}'''\n",
    "    article_df = article_df.reset_index(drop=True)\n",
    "    article_df['sen_length'] = article_df['sentence'].map(lambda x: len(x))\n",
    "\n",
    "    n_proc = 5 #cpu_count() #Number of CPU cores on your system\n",
    "\n",
    "    proc_chunks = get_proc_chunks(article_df, n_proc)\n",
    "    result_chunks = run(proc_chunks, 'w2v', art_entities_df)\n",
    "    art_vectors = pd.concat(result_chunks)\n",
    "\n",
    "    result_chunks = run(proc_chunks, 'get_entity', art_entities_df)\n",
    "    art_entities = result_chunks\n",
    "    # Fastest way to flatten list of arrays\n",
    "    art_vectors = list(itertools.chain(*art_vectors))\n",
    "    art_entities = list(itertools.chain(*art_entities))\n",
    "    art_entities = list(itertools.chain(*art_entities))\n",
    "    wordVector_Entity_df = pd.DataFrame({'wordVector': art_vectors, \n",
    "                                         'entityType': art_entities})\n",
    "\n",
    "    return wordVector_Entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demora: 1.6597187519073486\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                   strategy='exp_decay',\n",
    "                                                   W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('demora:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras: 4395\n",
      "Cantidad de entidades nombradas: 399\n",
      "Porcentaje de entidades nombradas: 9.08%\n"
     ]
    }
   ],
   "source": [
    "entity_list = list(wordVector_Entity_df['entityType'])\n",
    "tokens = len(entity_list)\n",
    "print('Cantidad de palabras: {}'.format(tokens))\n",
    "n_e = len(entity_list) - entity_list.count('O')\n",
    "print('Cantidad de entidades nombradas: {}'.format(n_e))\n",
    "print('Porcentaje de entidades nombradas: {:.2f}%'.format(n_e/tokens*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, cada artículo contiene una proporción reducida de palabras que son entidades.\n",
    "\n",
    "Una alternativa para evitar cómputo y uso de memoria podría ser eliminar una proporción de vectores que no representan ninguna entidad (etiquetados con 'O') a la hora de construir la matriz de palabra - entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_entities(df, frac):\n",
    "    '''\n",
    "    Remove a fraction of non entities vectors (entityType == 'O')\n",
    "    df: wordVector_Entity_df\n",
    "    frac: float value between 0 and 1\n",
    "    @return df with a fraction of the non entities rows removed\n",
    "    '''\n",
    "    sample = df[df.entityType == 'O'].sample(frac=frac, random_state=77)\n",
    "    df = df.drop(index=sample.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordVector</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.003621502994722617, 0.012132806459215266, ...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.08827082592044064, -0.13598437701614993, 0...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.16997208299104571, 0.0033914764131283102, ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.3915967207881669, -0.12179563739202309, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.048302400391625866, -0.09540661652931712, 0...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.5453598294447486, -0.15736036498200667, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.12074501704763917, 0.06705739051063955, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.014825387882357826, -0.25825100262032796, ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.10727509902624327, 0.04387436488263421, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.07985290764610813, -0.14920335335911, -0.03...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          wordVector entityType\n",
       "0  [-0.003621502994722617, 0.012132806459215266, ...        LOC\n",
       "1  [-0.08827082592044064, -0.13598437701614993, 0...        LOC\n",
       "2  [-0.16997208299104571, 0.0033914764131283102, ...          O\n",
       "3  [-0.3915967207881669, -0.12179563739202309, 0....          O\n",
       "4  [0.048302400391625866, -0.09540661652931712, 0...          O\n",
       "5  [-0.5453598294447486, -0.15736036498200667, 0....          O\n",
       "6  [-0.12074501704763917, 0.06705739051063955, 0....          O\n",
       "7  [-0.014825387882357826, -0.25825100262032796, ...          O\n",
       "8  [-0.10727509902624327, 0.04387436488263421, 0....          O\n",
       "9  [0.07985290764610813, -0.14920335335911, -0.03...          O"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVector_Entity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordVector</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.003621502994722617, 0.012132806459215266, ...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.08827082592044064, -0.13598437701614993, 0...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.5453598294447486, -0.15736036498200667, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.12074501704763917, 0.06705739051063955, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.10727509902624327, 0.04387436488263421, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.07985290764610813, -0.14920335335911, -0.03...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          wordVector entityType\n",
       "0  [-0.003621502994722617, 0.012132806459215266, ...        LOC\n",
       "1  [-0.08827082592044064, -0.13598437701614993, 0...        LOC\n",
       "5  [-0.5453598294447486, -0.15736036498200667, 0....          O\n",
       "6  [-0.12074501704763917, 0.06705739051063955, 0....          O\n",
       "8  [-0.10727509902624327, 0.04387436488263421, 0....          O\n",
       "9  [0.07985290764610813, -0.14920335335911, -0.03...          O"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_non_entities(wordVector_Entity_df.head(10), 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWordVectors_Entity(doc_filename, coarseNE_filename, strategy, W, w2v_model):\n",
    "    '''\n",
    "    Creates a N x D matrix of word vectors and saves it to disk. \n",
    "    Creates an 1 x N matrix of entities and saves it to disk. \n",
    "    N is the number of words in the document.\n",
    "    D is the size of each word vector.\n",
    "    The entity types are: PER - LOC - ORG - MISC - O\n",
    "    \n",
    "    strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "    W: window size\n",
    "    w2v_model: pre-trained word2vec model\n",
    "    '''\n",
    "    wordVectors = []\n",
    "    entityVector = []\n",
    "    count = 0 # Used in the 'progress bar'    \n",
    "    doc_df = pd.read_pickle('./corpus_WiNER/docs_df/'+ doc_filename)\n",
    "    coarseNE_df = pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ coarseNE_filename)\n",
    "    art_IDs = coarseNE_df.art_ID.unique()\n",
    "    \n",
    "    print('starting')\n",
    "    start = time.time()\n",
    "    print('doc_df shape:', doc_df.shape)\n",
    "    n_proc = 20 #cpu_count() #Number of CPU cores on your system\n",
    "    proc_chunks = get_proc_chunks(doc_df, n_proc)\n",
    "    print('len proc_chunks:', len(proc_chunks[0]))\n",
    "    result_chunks = run(proc_chunks, 'w2v', coarseNE_df)\n",
    "    print('finishing')\n",
    "    end = time.time()\n",
    "    print('demora:', end-start)\n",
    "    art_vectors = pd.concat(result_chunks)\n",
    "    print(type(art_vectors))\n",
    "    print('shapeee:', art_vectors.shape)\n",
    "    \n",
    "    \n",
    "    # We consider only the articles with at least one entity.\n",
    "    # That's why we iterate over the coarseNE's articles.\n",
    "    for art_ID in np.nditer(art_IDs):\n",
    "        article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "        art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]     \n",
    "        wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                           strategy, W, w2v_model)         \n",
    "        wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.90)\n",
    "        wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "        entityVector += list(wordVector_Entity_df['entityType'])\n",
    "\n",
    "        if count % 10 == 0: # Kind of progress bar :P\n",
    "            print(count, end=' ')\n",
    "        count += 1\n",
    "\n",
    "    starting = time.time()\n",
    "    np.savez_compressed('./corpus_WiNER/entity_vectors/ev_' + doc_filename + '_' + strategy,\n",
    "                        entityVector)\n",
    "    np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+ doc_filename + '_' + strategy,\n",
    "                        *wordVectors)\n",
    "    finishing = time.time()\n",
    "    print('tiempo de guardado:', finishing - starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "doc_df shape: (128395, 2)\n",
      "len proc_chunks: 6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing\n",
      "demora: 404.55298924446106\n",
      "<class 'pandas.core.series.Series'>\n",
      "shapeee: (128395,)\n",
      "0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-0097a9960ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m genWordVectors_Entity(doc_filename='doc_0', coarseNE_filename='coarseNE_0',\n\u001b[0;32m----> 3\u001b[0;31m                       strategy='exp_decay', W=5, w2v_model=w2v_model)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Demora total: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-fb08bcc532e2>\u001b[0m in \u001b[0;36mgenWordVectors_Entity\u001b[0;34m(doc_filename, coarseNE_filename, strategy, W, w2v_model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mart_entities_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoarseNE_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoarseNE_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mart_ID\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mart_ID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n\u001b[0;32m---> 41\u001b[0;31m                                                            strategy, W, w2v_model)         \n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mwordVector_Entity_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_non_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordVector_Entity_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwordVectors\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordVector_Entity_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wordVector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-1e5a3a595873>\u001b[0m in \u001b[0;36mgetVector_EntityFromArticle\u001b[0;34m(article_df, art_entities_df, strategy, W, w2v_model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mart_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresult_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_entities_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mart_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Fastest way to flatten list of arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-7cbcc84d5058>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(proc_chunks, f_name, art_entities_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m                         for chunk in proc_chunks]\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# blocks until all results are fetched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mresult_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0;31m# worker has not yet exited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaning up worker %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genWordVectors_Entity(doc_filename='doc_0', coarseNE_filename='coarseNE_0',\n",
    "                      strategy='exp_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = np.load('./corpus_WiNER/word_vectors/wv_doc_0_exp_decay_.npz')\n",
    "word_vectors = np.load('./corpus_WiNER/wv_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_0 300\n"
     ]
    }
   ],
   "source": [
    "for a_name, arr in word_vectors.iteritems():\n",
    "    print(a_name, len(arr))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_doc_0_exp_decay_.npz')\n",
    "entity_vector = np.load('./corpus_WiNER/ev_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_0 715816\n"
     ]
    }
   ],
   "source": [
    "for a_name, arr in entity_vector.iteritems():\n",
    "    print(a_name, len(arr))\n",
    "    entities = list(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296732"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.count('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createWordVectors_Entity(strategy, W):\n",
    "#     '''  TODO: change description!\n",
    "#     Create a N x (D+1) matrix where N is the total amount of words and \n",
    "#     D is the dimensionality of each word vector. \n",
    "#     The values of the last column are the entity type (PER - LOC - ORG - MISC - O).\n",
    "    \n",
    "#     strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "#     W: window size\n",
    "    \n",
    "#     @return wordVectors_Entity matrix\n",
    "#     '''\n",
    "#     w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')\n",
    "#     D = w2v_model.vector_size\n",
    "#     count = 0 # Used in the 'progress bar'\n",
    "#     doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "#     coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "#     doc_filenames.sort()\n",
    "#     coarseNE_filenames.sort()\n",
    "#     #TODO: remove the following after optimization\n",
    "#     starting = time.time()\n",
    "    \n",
    "    \n",
    "#     # TODO: modularizar: que el for interno sea otro metodo\n",
    "    \n",
    "#     for doc_f, coarseNE_f in zip(doc_filenames, coarseNE_filenames):\n",
    "#         wordVectors = []\n",
    "#         entityVector = []\n",
    "    \n",
    "#         start = time.time()\n",
    "        \n",
    "#         doc_df = pd.read_pickle('./corpus_WiNER/docs_df/'+ doc_f)\n",
    "#         coarseNE_df = pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ coarseNE_f)\n",
    "#         art_IDs = coarseNE_df.art_ID.unique()\n",
    "\n",
    "#         end = time.time()\n",
    "#         print('time 0: ', end-start)        \n",
    "        \n",
    "#         # We consider only the articles with at least one entity.\n",
    "#         # That's why we iterate over the coarseNE's articles.\n",
    "#         for art_ID in np.nditer(art_IDs):\n",
    "                        \n",
    "#             article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "#             art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]\n",
    "\n",
    "#             start = time.time()            \n",
    "#             wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "#                                                                strategy, D, W, w2v_model)\n",
    "#             end = time.time()\n",
    "#             print('wordVector_Entity_df: ', end-start) \n",
    "            \n",
    "            \n",
    "# #             start = time.time()            \n",
    "#             wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.90)\n",
    "# #             end = time.time()\n",
    "# #             print('dropping: ', end-start)                     \n",
    "            \n",
    "#             wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "#             entityVector += list(wordVector_Entity_df['entityType'])\n",
    "            \n",
    "# #             if count % 10 == 0:\n",
    "# #                 print(count, end=' ')\n",
    "# #             count += 1\n",
    "\n",
    "        \n",
    "#         finishing = time.time()\n",
    "#         print('tiempo de procesamiento:', finishing - starting)\n",
    "\n",
    "#         starting = time.time()\n",
    "#         np.savez_compressed('./corpus_WiNER/entity_vectors/ev_'+doc_f+'_'+strategy+'_', entityVector)\n",
    "#         np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+doc_f+'_'+strategy+'_', *wordVectors)\n",
    "\n",
    "#         finishing = time.time()\n",
    "#         print('tiempo de guardado:', finishing - starting)\n",
    "#         # Probamos sin el * => guardar en un array de arrays \n",
    "# #         np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+doc_f, wordVectors)\n",
    "      \n",
    "#         break\n",
    "#     print('End process')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
