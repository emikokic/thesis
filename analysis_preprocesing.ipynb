{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se explorarán, en pequeños experimentos, distintas formas de representación de los datos del corpus WiNER (Ghaddar y Langlais 2017) para utilizarlos en la tarea de reconocimiento de entidades nombradas. Para esto se exploran distintas combinaciones de vectores de palabras como representación de una instancia de entrenamiento (Iacobacci et al. 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del Corpus WiNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Documents.tar.bz2 : este archivo contiene 3239540 artículos de Wikipedia repartidos en 3223 archivos. Cantidad de oraciones: 54607542. Cada archivo contiene aproximadamente 1000 artículos nombrados por sus respectivos IDs. Los artículos están indexados por su wikiID seguidos de oraciones (una por línea), donde las palabras son remplazadas por sus ids.\n",
    "\n",
    "      ID <number>\n",
    "      1234 4522 23 4 4567\n",
    "      456 21 9890 123 7 0\n",
    "\n",
    "* document.vocab : este archivo contiene el mapeo de palabras (case sensitive); el formato es: \n",
    " \n",
    "      palabra #ocurrencias\n",
    "    \n",
    "  El ID de cada palabra es el número de línea en la cual ocurre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from corpus import spread_artID\n",
    "from embeddings import concat_vectors, mean_vectors, fractional_decay, exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = pd.read_csv('./corpus_WiNER/document.vocab', sep=' ', header=None, \n",
    "                           names=['word', 'frequency'], keep_default_na=False) \n",
    "                        # con esto evito que el parser trate como nan value a algunas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_mapping.shape)\n",
    "word_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_mapping['word'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los artículos del Documento \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = pd.read_csv('./corpus_WiNER/Documents/0', sep='ID', engine='python', header=None, names=['sentence', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a asociarle a cada oración el ID del artículo al cual pertenece.\n",
    "\n",
    "Es importante recordar que el orden de las oraciones está dado por los índices del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_ID_list = doc_0['art_ID'].tolist()\n",
    "art_ID = 0\n",
    "for idx, elem in enumerate(art_ID_list):\n",
    "    if not np.isnan(elem):\n",
    "        art_ID = elem\n",
    "    else:\n",
    "        art_ID_list[idx] = art_ID\n",
    "doc_0['art_ID'] = art_ID_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos ahora las filas con NaN que contenian los ID de los artículos inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = doc_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El documento contiene {} oraciones.'.format(doc_0.shape[0]))\n",
    "print('El documento contiene {} artículos'.format(len(doc_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos que cada sentencia sea una lista de palabras codificadas y casteamos a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0['sentence'] = doc_0['sentence'].map(lambda x: list(map(int, x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con un artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruimos la primera oración del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_decoder(sentence, word_mapping):\n",
    "    dec_sentence = []\n",
    "    for idx in sentence:\n",
    "        mapped_w = word_mapping.loc[idx, 'word']\n",
    "        dec_sentence.append(mapped_w)\n",
    "    return dec_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos article.columns.get_loc('sentence') para evitar hardcodear el índice correspondiente\n",
    "# a la columna 'sentence' que en este caso es 0\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings utilizando el modelo pre-entrenado word2vec de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la librería Gensim https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos Google's pre-trained Word2Vec model.\n",
    "\n",
    "Utilizando KeyedVectors para cargar el modelo tiene la desventaja de que no se puede seguir entrenando. Pero es más eficiente que utilizar gensim.models.Word2Vec\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demora: 35.06152105331421\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# model = KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save('./models/word2vecGoogle.model')\n",
    "w2v_model = KeyedVectors.load('./models/word2vecGoogle.model')\n",
    "end = time.time()\n",
    "print('demora: {}'.format(end-start))\n",
    "# model = Word2Vec.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de word embeddings: 3000000\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de word embeddings: {}'.format(len(w2v_model.vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de los vectores: 300\n"
     ]
    }
   ],
   "source": [
    "print('Dimensionalidad de los vectores: {}'.format(w2v_model.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos distintas combinaciones de vectores de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación\n",
    "\n",
    "Este método consiste en concatenar los vectores de palabras que rodean una palabra objetivo en un vector más grande, que tiene un tamaño igual a las dimensiones agregadas de todos las proyecciones (embeddings) individuales.\n",
    "\n",
    "- $w_{ij}$ = peso asociado con la i-ésima dimensión del vector de la j-ésima palabra en la oración. NOTA: con los vectores de palabras de una oración se forma una matriz $w^{\\space D\\space x\\space L}$ donde $L$ es la cantidad de palabras de esa oración.\n",
    "- $D$ = dimensionalidad de los word vectors originales. Por ejemplo, al usar el modelo word2vec de Google se tiene $D$ = 300.\n",
    "- $W$ = tamaño de ventana que se define como el número de palabras en un solo lado.\n",
    "\n",
    "Nos interesa representar el contexto de la I-ésima palabra de la oración. \n",
    "\n",
    "La i-ésima dimensión del vector de concatenación, que tiene un tamaño de $2 W D$, se calcula de la siguiente manera:\n",
    "\n",
    "$$ e_{i} =\\begin{cases} \n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & \\left\\lfloor{\\frac{i}{D}}\\right\\rfloor < W \\\\\n",
    "      w_{i\\space mod \\space D,\\space\\space I \\space - \\space W \\space + \\space 1\\space  +\\space\\left\\lfloor{\\frac{i}{D}}\\right\\rfloor} & c.c.\n",
    "   \\end{cases}$$\n",
    "   \n",
    "\n",
    "<br>\n",
    "Al tomar una ventana simétrica, se realiza un relleno (padding) con ceros a izquierda y/o derecha según corresponda para mantener la misma dimesionalidad en cada nuevo vector generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promedio\n",
    "\n",
    "Como su nombre indica, se calcula el centroide de los embeddings de todas las palabras circundantes. La fórmula divide cada dimensión en $2W$ ya que el número de palabras del contexto es dos veces el tamaño de la ventana:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} \\frac{w_{ij}}{2W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento fraccional\n",
    "\n",
    "Una tercera estrategia para construir un vector de carácteristicas en base a los embeddings de palabras contextuales está inspirada en la forma en que Word2vec combina las palabras en el contexto. Aquí, se supone que la importancia de una palabra para nuestra representación es inversamente proporcional a su distancia respecto a la palabra objetivo.\n",
    "\n",
    "Por lo tanto, las palabras contextuales se ponderan en función de su distancia de la palabra objetivo:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} *\\frac{W - \\lvert I-j\\rvert}{W}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaimiento exponencial\n",
    "\n",
    "Funciona de manera similar al decaimiento fraccional, que le da más importancia al contexto cercano, pero en este caso la ponderación se realiza exponencialmente:\n",
    "\n",
    "$$e_{i} =\\sum_{\\substack{j\\space=\\space I-W \\\\ j\\space\\neq\\space I}}^{I + W} w_{ij} * (1 - \\alpha)^{\\lvert \\space I\\space-\\space j\\space\\rvert\\space-\\space1}$$\n",
    "\n",
    "donde $\\alpha = 1 - 0.1^{(W-1)^{-1}}$ es el parámetro de decaimiento. Elegimos el parámetro de tal manera que las palabras inmediatas que rodean a la palabra objetivo contribuyen 10 veces más que las últimas palabras en ambos lados de la ventana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos CoarseNE.tar.bz2\n",
    "\n",
    "Contiene menciones anotadas automáticamente con etiquetas de entidades nombradas (PER, LOC, ORG y MISC).\n",
    "\n",
    "El formato es:\n",
    "\n",
    "    ID artID\n",
    "    sentIdx begin end entityType\n",
    "    \n",
    "donde entityType[0] = PER | entityType[1] = LOC | entityType[2] = ORG | entityType[3] = MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = pd.read_csv('./corpus_WiNER/CoarseNE/0', sep='ID', engine='python', header=None, names=['named-entity', 'art_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coarseNE_0.shape)\n",
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el mismo truco que utilizamos en los documentos para propagar los art_ID según corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0 = spread_artID(coarseNE_0)\n",
    "coarseNE_0 = coarseNE_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coarseNE_0 contiene {} entidades.'.format(coarseNE_0.shape[0]))\n",
    "print('coarseNE_0 contiene {} artículos'.format(len(coarseNE_0['art_ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuevas columnas con la info de la columna named-entity para mejor manipulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(x):\n",
    "    tags = ['PER', 'LOC', 'ORG', 'MISC']\n",
    "    return tags[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0['named-entity'] = coarseNE_0['named-entity'].map(lambda x: x.split('\\t'))\n",
    "coarseNE_0['senIdx'] = coarseNE_0['named-entity'].map(lambda x: int(x[0]))\n",
    "coarseNE_0['begin'] = coarseNE_0['named-entity'].map(lambda x: int(x[1]))\n",
    "coarseNE_0['end'] = coarseNE_0['named-entity'].map(lambda x: int(x[2]))\n",
    "coarseNE_0['entityType'] = coarseNE_0['named-entity'].map(lambda x: get_tag(int(x[3])))\n",
    "coarseNE_0 = coarseNE_0.drop(columns='named-entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseNE_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los artículos que ocurren en Documents/0 se encuentran en CoarseNE/0\n",
    "\n",
    "Notar que esto sucede porque esos artículos no contienen entidades nombradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Artículos que están presentes en Documents/0 pero no en CoarseNE/0: {}'\n",
    "      .format(list(set(doc_0.art_ID.unique()) - set(coarseNE_0.art_ID.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 431177]\n",
    "for sentence in article.sentence.values:\n",
    "    dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "    print(' '.join(dec_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos como están anotadas las entidades nombradas de una oración en particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = doc_0[doc_0.art_ID == 1000]\n",
    "sentence = article.iloc[0, article.columns.get_loc('sentence')]\n",
    "dec_sentence = sentence_decoder(sentence, word_mapping)\n",
    "' '.join(dec_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_entities = coarseNE_0[coarseNE_0.art_ID == 1000]\n",
    "entities_sen_0 = art_entities[art_entities.senIdx == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in entities_sen_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora utilicemos los dataframes preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>art_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Belton, House, is, a, Grade, I, listed, count...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, mansion, is, surrounded, by, formal, gar...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Belton, has, been, described, as, a, compilat...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, house, has, also, been, described, as, t...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Only, Brympton, d'Evercy, has, been, similarl...</td>\n",
       "      <td>145492.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    art_ID\n",
       "1  [Belton, House, is, a, Grade, I, listed, count...  145492.0\n",
       "2  [The, mansion, is, surrounded, by, formal, gar...  145492.0\n",
       "3  [Belton, has, been, described, as, a, compilat...  145492.0\n",
       "4  [The, house, has, also, been, described, as, t...  145492.0\n",
       "5  [Only, Brympton, d'Evercy, has, been, similarl...  145492.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc77_df = pd.read_pickle('./corpus_WiNER/docs_df/doc_77')\n",
    "article_df = doc77_df[doc77_df.art_ID == 145492]\n",
    "dec_sentence = article_df.iloc[0, article_df.columns.get_loc('sentence')]\n",
    "doc77_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_ID</th>\n",
       "      <th>senIdx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42515</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42516</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42517</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42518</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42519</th>\n",
       "      <td>145492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         art_ID  senIdx  begin  end entityType\n",
       "42515  145492.0       0      0    2        LOC\n",
       "42516  145492.0       0     10   11        LOC\n",
       "42517  145492.0       0     12   13        LOC\n",
       "42518  145492.0       0     14   15        LOC\n",
       "42519  145492.0       0     16   17        LOC"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseNE_77 = pd.read_pickle('./corpus_WiNER/coarseNE_df/coarseNE_77')\n",
    "art_entities_df = coarseNE_77[coarseNE_77.art_ID == 145492]\n",
    "sen_entities_0 = art_entities_df[art_entities_df.senIdx == 0]\n",
    "art_entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityListFromSentence(senIdx, sen_length, art_entities_df):\n",
    "    # We take the df with the entities of each sentence\n",
    "    sen_entities_df = art_entities_df[art_entities_df.senIdx == senIdx]\n",
    "    # An empty dataframe means that the sentence doesn't have any entity\n",
    "    if sen_entities_df.empty:\n",
    "        entities = ['O' for _ in range(sen_length)]\n",
    "    else:\n",
    "        entities = []\n",
    "        i = 0\n",
    "        for _, row in sen_entities_df.iterrows():\n",
    "            while i < row['begin']:\n",
    "                entities.append('O')\n",
    "                i += 1\n",
    "            while i < row['end']:\n",
    "                entities.append(row['entityType'])\n",
    "                i += 1\n",
    "        while i < sen_length:\n",
    "            entities.append('O')\n",
    "            i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belton House : LOC\n",
      "Belton : LOC\n",
      "Grantham : LOC\n",
      "Lincolnshire : LOC\n",
      "England : LOC\n"
     ]
    }
   ],
   "source": [
    "for idx, row in sen_entities_0.iterrows():\n",
    "    print('{} : {}'.format(' '.join(dec_sentence[row['begin']:row['end']]), row['entityType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belton House is a Grade I listed country house in Belton near Grantham , Lincolnshire , England .\n",
      "['LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'LOC', 'O', 'LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "entity_list = entityListFromSentence(senIdx=0, sen_length=len(dec_sentence),\n",
    "                                     art_entities_df=art_entities_df)\n",
    "print(' '.join(article_df['sentence'][1]))\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_strategy(strategy, sentence, W, w2v_model):\n",
    "    return {\n",
    "        'concat': concat_vectors(sentence, W, w2v_model), \n",
    "        'mean': mean_vectors(sentence, W, w2v_model), \n",
    "        'frac_decay': fractional_decay(sentence, W, w2v_model),\n",
    "        'exp_decay': exponential_decay(sentence, W, w2v_model),\n",
    "    }[strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector_EntityFromArticle(article_df, art_entities_df, strategy, W, w2v_model):\n",
    "    '''@return: filled DataFrame with columns {wordVector, entityType}'''\n",
    "    article_df = article_df.reset_index(drop=True)\n",
    "    article_df['sen_length'] = article_df['sentence'].map(lambda x: len(x))\n",
    "\n",
    "    fun = lambda sentence: w2v_strategy(strategy, sentence, W, w2v_model)\n",
    "    article_df['sen_vectors'] = article_df['sentence'].map(fun)\n",
    "    art_vectors = list(article_df['sen_vectors'])\n",
    "\n",
    "    fun2 = lambda senIdx: entityListFromSentence(senIdx, article_df.loc[senIdx, 'sen_length'],\n",
    "                                                 art_entities_df)\n",
    "    article_df['sen_entities'] = article_df.index.map(fun2)\n",
    "    art_entities = list(article_df['sen_entities'])\n",
    "    # Fastest way to flatten list of arrays\n",
    "    art_vectors = list(itertools.chain(*art_vectors))\n",
    "    art_entities = list(itertools.chain(*art_entities))\n",
    "\n",
    "    wordVector_Entity_df = pd.DataFrame({'wordVector': art_vectors, \n",
    "                                         'entityType': art_entities})\n",
    "\n",
    "    return wordVector_Entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demora: 10.182647228240967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                   strategy='exp_decay',\n",
    "                                                   W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('demora:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras: 4395\n",
      "Cantidad de entidades nombradas: 399\n",
      "Porcentaje de entidades nombradas: 9.08%\n"
     ]
    }
   ],
   "source": [
    "entity_list = list(wordVector_Entity_df['entityType'])\n",
    "tokens = len(entity_list)\n",
    "print('Cantidad de palabras: {}'.format(tokens))\n",
    "n_e = len(entity_list) - entity_list.count('O')\n",
    "print('Cantidad de entidades nombradas: {}'.format(n_e))\n",
    "print('Porcentaje de entidades nombradas: {:.2f}%'.format(n_e/tokens*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, cada artículo contiene una proporción reducida de palabras que son entidades.\n",
    "\n",
    "Una alternativa para evitar cómputo y uso de memoria podría ser eliminar una proporción de vectores que no representan ninguna entidad (etiquetados con 'O') a la hora de construir la matriz de palabra - entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_entities(df, frac):\n",
    "    '''\n",
    "    Remove a fraction of non entities vectors (entityType == 'O')\n",
    "    df: wordVector_Entity_df\n",
    "    frac: float value between 0 and 1\n",
    "    @return df with a fraction of the non entities rows removed\n",
    "    '''\n",
    "    sample = df[df.entityType == 'O'].sample(frac=frac, random_state=77)\n",
    "    df = df.drop(index=sample.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordVector</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.003621502994722617, 0.012132806459215266, ...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.08827082592044064, -0.13598437701614993, 0...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.16997208299104571, 0.0033914764131283102, ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.3915967207881669, -0.12179563739202309, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.048302400391625866, -0.09540661652931712, 0...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.5453598294447486, -0.15736036498200667, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.12074501704763917, 0.06705739051063955, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.014825387882357826, -0.25825100262032796, ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.10727509902624327, 0.04387436488263421, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.07985290764610813, -0.14920335335911, -0.03...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          wordVector entityType\n",
       "0  [-0.003621502994722617, 0.012132806459215266, ...        LOC\n",
       "1  [-0.08827082592044064, -0.13598437701614993, 0...        LOC\n",
       "2  [-0.16997208299104571, 0.0033914764131283102, ...          O\n",
       "3  [-0.3915967207881669, -0.12179563739202309, 0....          O\n",
       "4  [0.048302400391625866, -0.09540661652931712, 0...          O\n",
       "5  [-0.5453598294447486, -0.15736036498200667, 0....          O\n",
       "6  [-0.12074501704763917, 0.06705739051063955, 0....          O\n",
       "7  [-0.014825387882357826, -0.25825100262032796, ...          O\n",
       "8  [-0.10727509902624327, 0.04387436488263421, 0....          O\n",
       "9  [0.07985290764610813, -0.14920335335911, -0.03...          O"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVector_Entity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordVector</th>\n",
       "      <th>entityType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.003621502994722617, 0.012132806459215266, ...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.08827082592044064, -0.13598437701614993, 0...</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.5453598294447486, -0.15736036498200667, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.12074501704763917, 0.06705739051063955, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.10727509902624327, 0.04387436488263421, 0....</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.07985290764610813, -0.14920335335911, -0.03...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          wordVector entityType\n",
       "0  [-0.003621502994722617, 0.012132806459215266, ...        LOC\n",
       "1  [-0.08827082592044064, -0.13598437701614993, 0...        LOC\n",
       "5  [-0.5453598294447486, -0.15736036498200667, 0....          O\n",
       "6  [-0.12074501704763917, 0.06705739051063955, 0....          O\n",
       "8  [-0.10727509902624327, 0.04387436488263421, 0....          O\n",
       "9  [0.07985290764610813, -0.14920335335911, -0.03...          O"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_non_entities(wordVector_Entity_df.head(10), 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWordVectors_Entity(doc_filename, coarseNE_filename, strategy, W, w2v_model):\n",
    "    '''\n",
    "    Creates a N x D matrix of word vectors and saves it to disk. \n",
    "    Creates an 1 x N matrix of entities and saves it to disk. \n",
    "    N is the number of words in the document.\n",
    "    D is the size of each word vector.\n",
    "    The entity types are: PER - LOC - ORG - MISC - O\n",
    "    \n",
    "    strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "    W: window size\n",
    "    w2v_model: pre-trained word2vec model\n",
    "    '''\n",
    "    wordVectors = []\n",
    "    entityVector = []\n",
    "    count = 0 # Used in the 'progress bar'    \n",
    "    doc_df = pd.read_pickle('./corpus_WiNER/docs_df/'+ doc_filename)\n",
    "    coarseNE_df = pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ coarseNE_filename)\n",
    "    art_IDs = coarseNE_df.art_ID.unique()      \n",
    "    # We consider only the articles with at least one entity.\n",
    "    # That's why we iterate over the coarseNE's articles.\n",
    "    for art_ID in np.nditer(art_IDs):\n",
    "        article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "        art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]     \n",
    "        wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                           strategy, W, w2v_model)         \n",
    "        wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.90)\n",
    "        wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "        entityVector += list(wordVector_Entity_df['entityType'])\n",
    "\n",
    "        if count % 10 == 0: # Kind of progress bar :P\n",
    "            print(count, end=' ')\n",
    "        count += 1\n",
    "\n",
    "    starting = time.time()\n",
    "    np.savez_compressed('./corpus_WiNER/entity_vectors/ev_' + doc_filename + '_' + strategy,\n",
    "                        entityVector)\n",
    "    np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+ doc_filename + '_' + strategy,\n",
    "                        *wordVectors)\n",
    "    finishing = time.time()\n",
    "    print('tiempo de guardado:', finishing - starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# genWordVectors_Entity(doc_filename='doc_0', coarseNE_filename='coarseNE_0',\n",
    "#                       strategy='exp_decay', W=5, w2v_model=w2v_model)\n",
    "# end = time.time()\n",
    "# print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = np.load('./corpus_WiNER/word_vectors/wv_doc_0_exp_decay_.npz')\n",
    "# word_vectors = np.load('./corpus_WiNER/wv_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a_name, arr in word_vectors.iteritems():\n",
    "#     print(a_name, len(arr))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_vector = np.load('./corpus_WiNER/entity_vectors/ev_doc_0_exp_decay_.npz')\n",
    "# entity_vector = np.load('./corpus_WiNER/ev_doc_0_exp_decay_.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a_name, arr in entity_vector.iteritems():\n",
    "#     print(a_name, len(arr))\n",
    "#     entities = list(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities.count('O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# División de los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los primeros 1500 documentos que se van a utilizar para la parte supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "doc_filenames.sort()\n",
    "doc_filenames = doc_filenames[0:1500]\n",
    "coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "coarseNE_filenames.sort()\n",
    "coarseNE_filenames = coarseNE_filenames[0:1500]\n",
    "docs = []\n",
    "coarseNEs = []\n",
    "for doc, ne in zip(doc_filenames, coarseNE_filenames):\n",
    "    docs.append(pd.read_pickle('./corpus_WiNER/docs_df/'+ doc))\n",
    "    coarseNEs.append(pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ ne))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.concat(docs, ignore_index=True)\n",
    "coarseNE_df = pd.concat(coarseNEs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de oraciones: 23811536\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de oraciones:', docs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de artículos que no contienen ninguna entidad: 3449\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de artículos que no contienen ninguna entidad:', \n",
    "      len(docs_df.art_ID.unique()) - len(coarseNE_df.art_ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con aquellos artículos que contienen al menos una entidad nombrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_IDs = coarseNE_df.art_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraemos una muestra aleatoria de 4000 artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # No funciona ??\n",
    "np.random.shuffle(art_IDs)\n",
    "art_IDs_sample = art_IDs[0:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = docs_df[docs_df.art_ID.isin(art_IDs_sample)]\n",
    "entities_df = coarseNE_df[coarseNE_df.art_ID.isin(art_IDs_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65993, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.shape # Por mas que se setea el seed el sample va cambiando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWordVectors_Entity(doc_df, coarseNE_df, strategy, W, w2v_model):\n",
    "    '''\n",
    "    Creates a N x D matrix of word vectors and saves it to disk. \n",
    "    Creates an 1 x N matrix of entities and saves it to disk. \n",
    "    N is the number of words in the document.\n",
    "    D is the size of each word vector.\n",
    "    The entity types are: PER - LOC - ORG - MISC - O\n",
    "    \n",
    "    strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "    W: window size\n",
    "    w2v_model: pre-trained word2vec model\n",
    "    '''\n",
    "    wordVectors = []\n",
    "    entityVector = []   \n",
    "    art_IDs = coarseNE_df.art_ID.unique()      \n",
    "    # We consider only the articles with at least one entity.\n",
    "    # That's why we iterate over the coarseNE's articles.\n",
    "    for art_ID in tqdm(np.nditer(art_IDs)):\n",
    "        article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "        art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]     \n",
    "        wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                           strategy, W, w2v_model)         \n",
    "        wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.80)\n",
    "        wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "        entityVector += list(wordVector_Entity_df['entityType'])\n",
    "\n",
    "    starting = time.time()\n",
    "    np.savez_compressed('./corpus_WiNER/entity_vectors/ev_' + 'sample' + '_' + strategy \n",
    "                        + '_W_' + str(W), entityVector)\n",
    "    np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+ 'sample' + '_' + strategy\n",
    "                        + '_W_' + str(W), wordVectors)\n",
    "    finishing = time.time()\n",
    "    print('tiempo de guardado:', finishing - starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 3400 3410 3420 3430 3440 3450 3460 3470 3480 3490 3500 3510 3520 3530 3540 3550 3560 3570 3580 3590 3600 3610 3620 3630 3640 3650 3660 3670 3680 3690 3700 3710 3720 3730 3740 3750 3760 3770 3780 3790 3800 3810 3820 3830 3840 3850 3860 3870 3880 3890 3900 3910 3920 3930 3940 3950 3960 3970 3980 3990 tiempo de guardado: 58.77945160865784\n",
      "Demora total: 1209.6786572933197\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='exp_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 3400 3410 3420 3430 3440 3450 3460 3470 3480 3490 3500 3510 3520 3530 3540 3550 3560 3570 3580 3590 3600 3610 3620 3630 3640 3650 3660 3670 3680 3690 3700 3710 3720 3730 3740 3750 3760 3770 3780 3790 3800 3810 3820 3830 3840 3850 3860 3870 3880 3890 3900 3910 3920 3930 3940 3950 3960 3970 3980 3990 tiempo de guardado: 103.9373607635498\n",
      "Demora total: 1266.6579024791718\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='frac_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 3400 3410 3420 3430 3440 3450 3460 3470 3480 3490 3500 3510 3520 3530 3540 3550 3560 3570 3580 3590 3600 3610 3620 3630 3640 3650 3660 3670 3680 3690 3700 3710 3720 3730 3740 3750 3760 3770 3780 3790 3800 3810 3820 3830 3840 3850 3860 3870 3880 3890 3900 3910 3920 3930 3940 3950 3960 3970 3980 3990 tiempo de guardado: 94.46472907066345\n",
      "Demora total: 1163.3876712322235\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='mean', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# genWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "#                       strategy='concat', W=5, w2v_model=w2v_model)\n",
    "# end = time.time()\n",
    "# print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los segundos 1500 documentos que se van a utilizar para la parte no supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_filenames = os.listdir('./corpus_WiNER/docs_df/')\n",
    "doc_filenames.sort()\n",
    "doc_filenames = doc_filenames[1500:3000]\n",
    "coarseNE_filenames = os.listdir('./corpus_WiNER/coarseNE_df/')\n",
    "coarseNE_filenames.sort()\n",
    "coarseNE_filenames = coarseNE_filenames[1500:3000]\n",
    "docs = []\n",
    "coarseNEs = []\n",
    "for doc, ne in zip(doc_filenames, coarseNE_filenames):\n",
    "    docs.append(pd.read_pickle('./corpus_WiNER/docs_df/'+ doc))\n",
    "    coarseNEs.append(pd.read_pickle('./corpus_WiNER/coarseNE_df/'+ ne))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.concat(docs, ignore_index=True)\n",
    "coarseNE_df = pd.concat(coarseNEs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de oraciones: 25935452\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de oraciones:', docs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de artículos que no contienen ninguna entidad: 2732\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de artículos que no contienen ninguna entidad:', \n",
    "      len(docs_df.art_ID.unique()) - len(coarseNE_df.art_ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con aquellos artículos que contienen al menos una entidad nombrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_IDs = coarseNE_df.art_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraemos una muestra aleatoria de 12000 artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # No funciona ??\n",
    "np.random.shuffle(art_IDs)\n",
    "art_IDs_sample = art_IDs[0:12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = docs_df[docs_df.art_ID.isin(art_IDs_sample)]\n",
    "entities_df = coarseNE_df[coarseNE_df.art_ID.isin(art_IDs_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201934, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.shape # Por mas que se setea el seed el sample va cambiando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genNonSupervisedWordVectors_Entity(doc_df, coarseNE_df, strategy, W, w2v_model):\n",
    "    '''\n",
    "    Creates a N x D matrix of word vectors and saves it to disk. \n",
    "    Creates an 1 x N matrix of entities and saves it to disk. \n",
    "    N is the number of words in the document.\n",
    "    D is the size of each word vector.\n",
    "    The entity types are: PER - LOC - ORG - MISC - O\n",
    "    \n",
    "    strategy: 'concat', 'mean', 'frac_decay', 'exp_decay'.\n",
    "    W: window size\n",
    "    w2v_model: pre-trained word2vec model\n",
    "    '''\n",
    "    wordVectors = []\n",
    "    entityVector = []   \n",
    "    art_IDs = coarseNE_df.art_ID.unique()      \n",
    "    # We consider only the articles with at least one entity.\n",
    "    # That's why we iterate over the coarseNE's articles.\n",
    "    for art_ID in tqdm(np.nditer(art_IDs)):\n",
    "        article_df = doc_df[doc_df.art_ID == art_ID]\n",
    "        art_entities_df = coarseNE_df[coarseNE_df.art_ID == art_ID]     \n",
    "        wordVector_Entity_df = getVector_EntityFromArticle(article_df, art_entities_df, \n",
    "                                                           strategy, W, w2v_model)         \n",
    "        wordVector_Entity_df = drop_non_entities(wordVector_Entity_df, 0.80)\n",
    "        wordVectors += list(wordVector_Entity_df['wordVector'])         \n",
    "        entityVector += list(wordVector_Entity_df['entityType'])\n",
    "\n",
    "    starting = time.time()\n",
    "    np.savez_compressed('./corpus_WiNER/entity_vectors/ev_' + 'no_supervised' + '_' + strategy \n",
    "                        + '_W_' + str(W), entityVector)\n",
    "    np.savez_compressed('./corpus_WiNER/word_vectors/wv_'+ 'no_supervised' + '_' + strategy\n",
    "                        + '_W_' + str(W), wordVectors)\n",
    "    finishing = time.time()\n",
    "    print('tiempo de guardado:', finishing - starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [01:20,  1.44it/s]                     /users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "12000it [3:18:01,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo de guardado: 672.1255185604095\n",
      "Demora total: 12557.627238988876\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genNonSupervisedWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='exp_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [01:31,  1.27it/s]                     /users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "12000it [3:29:47,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo de guardado: 1298.1283988952637\n",
      "Demora total: 13889.372932195663\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genNonSupervisedWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='frac_decay', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [01:17,  1.48it/s]                     /users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/users/ekokic/thesis_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "12000it [3:29:09,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo de guardado: 1099.0262243747711\n",
      "Demora total: 13652.744041204453\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "genNonSupervisedWordVectors_Entity(doc_df=articles_df, coarseNE_df=entities_df,\n",
    "                      strategy='mean', W=5, w2v_model=w2v_model)\n",
    "end = time.time()\n",
    "print('Demora total: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
